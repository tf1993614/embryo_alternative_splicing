{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "b4e41870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from functools import reduce\n",
    "from multiprocessing import Process\n",
    "from scipy.stats import rankdata, gmean\n",
    "\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE,VarianceThreshold,SelectKBest,chi2, SelectFromModel, f_classif, mutual_info_classif\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dad5100b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(path, pattern, recursive = True, full_name = True):\n",
    "    output = []\n",
    "    def list_files_core(current_path = path, current_pattern = pattern, current_recursive = recursive, current_full_name = full_name):\n",
    "        nonlocal output\n",
    "        files = os.listdir(current_path)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(current_path, file)\n",
    "            \n",
    "            if os.path.isdir(file_path) and current_recursive:\n",
    "                list_files_core(file_path, current_pattern, current_recursive, current_full_name)\n",
    "            \n",
    "            else:\n",
    "                if re.search(current_pattern, file):\n",
    "                    if full_name == True:\n",
    "                        file = os.path.join(current_path, file)\n",
    "                        output.append(file)\n",
    "                    else:\n",
    "                        output.append(file)\n",
    "    list_files_core()\n",
    "    return output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "065e17f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(data = None, path = None, assay = \".X\",label_column = None):\n",
    "    \"\"\"\n",
    "    A function to convert sparse matrix into pandas data frame object. \n",
    "    \n",
    "    Parameters:\n",
    "    data: a Annot Data object. If speciefied, path should be specified in 'None'.\n",
    "    path: path to h5ad file. If speciefied, data should be specified in 'None'.\n",
    "    assay: \".X\" or \"raw\" assay in Annot Data object could be specified\n",
    "    label_column: the name of cell type column in the h5ad file. \n",
    "    If specified, the cell type column will be added into output.\n",
    "    \"\"\"\n",
    "    if path != None:\n",
    "        data = sc.read(path)\n",
    "    \n",
    "    if assay == \".X\":\n",
    "        counts = pd.DataFrame.sparse.from_spmatrix(data.X)\n",
    "    else:\n",
    "        counts = pd.DataFrame.sparse.from_spmatrix(data.raw.X)\n",
    "        \n",
    "    features = data.raw.var_names.tolist()\n",
    "    index  = data.raw.obs_names.tolist()\n",
    "    \n",
    "    counts.columns = features\n",
    "    counts.index = index\n",
    "    \n",
    "    if label_column != None:\n",
    "        try:\n",
    "            labels = data.obs[label_column].tolist()\n",
    "            counts[\"cell_type\"] = labels\n",
    "        except:\n",
    "            raise ValueError(\"The length of cell type column is not consistent with matrix\")\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00060a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_normalize(data):\n",
    "    \"\"\"\n",
    "    A function to do quantile normalization.  \n",
    "    \n",
    "    \"\"\"\n",
    "    data = data.loc[:, data.columns != \"cell_type\"]\n",
    "    ranks = (data.rank(method = \"first\").stack())\n",
    "    rank_mean = (data.stack().groupby(ranks).mean())\n",
    "    # add interproblated values in between ranks\n",
    "    finer_ranks = ((rank_mean.index + 0.5).tolist() + rank_mean.index.tolist())\n",
    "    rank_mean = rank_mean.reindex(finer_ranks).sort_index().interpolate()\n",
    "    data = data.rank(method = \"average\").stack().map(rank_mean).unstack()\n",
    "    \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4efc3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_time():\n",
    "    \"\"\"\n",
    "    A function to call out current time.\n",
    "    \"\"\"\n",
    "    current_second_time = datetime.now()\n",
    "    return current_second_time.strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57899021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class log_file:\n",
    "    \"\"\"\n",
    "    A class to easily write log information into different log files.\n",
    "    \n",
    "    filename: log file name \n",
    "    mode: \"a\" (append log information) or \"w\" (clean up old log information and then write new information)\n",
    "    \"\"\"\n",
    "    n_class = 0\n",
    "    def __init__(self, filename, mode):\n",
    "        self.time = record_time()\n",
    "        self.filename = filename\n",
    "        self.mode = mode\n",
    "        self.logger = None\n",
    "        self.n_object = 0\n",
    "        log_file.n_class += 1\n",
    "        \n",
    "    def set(self):\n",
    "        logger = logging.getLogger(\"my_logger\")\n",
    "        logger.setLevel(logging.INFO)\n",
    "        log_file_name = \"{}_{}.log\".format(self.filename, self.time)\n",
    "        file_handler = logging.FileHandler(log_file_name, mode = self.mode)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "        self.logger = logger\n",
    "        \n",
    "    def start(self):\n",
    "        if log_file.n_class == 1 and self.n_object == 0:\n",
    "            self.set()\n",
    "        if log_file.n_class > 1 and self.n_object == 0:\n",
    "            self.set()\n",
    "        \n",
    "    def write(self, category, content):\n",
    "        self.start()\n",
    "        self.n_object += 1\n",
    "        if category == \"info\":\n",
    "            self.logger.info(content)\n",
    "        elif category == \"error\":\n",
    "            self.logger.error(content)\n",
    "        elif category == \"warning\":\n",
    "            self.logger.warning(content)\n",
    "        elif category == \"debug\":\n",
    "            self.logger.debug(content)\n",
    "        elif category == \"critical\":\n",
    "            self.logger.critical(content)\n",
    "    \n",
    "    def mode_reset(self, mode):\n",
    "        self.mode = mode\n",
    "        \n",
    "    def clear():\n",
    "        log_file.n_class = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c745cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_time(string):\n",
    "    \"\"\"\n",
    "    A function to remove time information in the stdout message\"\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\")\n",
    "    res = re.sub(pattern, \"\", string)\n",
    "    return res.rstrip().lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cea2ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(path, \n",
    "                   label_column,\n",
    "                   assay = \".X\", \n",
    "                   convert = True,\n",
    "                   log_normalize = True, \n",
    "                   scale_data = False, \n",
    "                   quantile_normalize = False,\n",
    "                   save = False,\n",
    "                   logger = None):\n",
    "    \"\"\"\n",
    "    A function to do preliminary normalization, encoding label and convert data into pandas data frame for downstream sklearn-based\n",
    "    machine learning workflow.\n",
    "    \n",
    "    Parameters:\n",
    "    path: path to the h5ad file.\n",
    "    assay: \".X\" or \"raw\" assay in Annot Data object could be specified.Default is \".X\".\n",
    "    label_column: The name of cell type column in the h5ad file.If specified, the cell type column will be added into output.\n",
    "    convert: Bool value to decide whether convert Annot Data object into pandas data frame object.\n",
    "    log_normalize: Bool value to decide whether standard log normalization to be done.\n",
    "    scale_data: Bool value to decide whether standadlize data or not.\n",
    "    quantile_normalize: Bool value to decide whether quantile normalize data or not.\n",
    "    save: Bool value to decide whether write the pre-processed data into the disk.\n",
    "    logger: A log_file object to write log information into disk. Default is None. \n",
    "    \"\"\"\n",
    "    if logger != \"None\":\n",
    "        logger.write(\"info\", \"start pre processing\")\n",
    "        logger.write(\"critical\", \"Parameters used for pre_processing\")\n",
    "        parameters = {\"path\": path,\n",
    "                      \"label_column\": label_column,\n",
    "                      \"assay\": assay,\n",
    "                      \"convert\": convert,\n",
    "                      \"log_normalize\": log_normalize,\n",
    "                      \"scale_data\": scale_data,\n",
    "                      \"quantile_normalize\": quantile_normalize,\n",
    "                      \"save\": save\n",
    "                     }\n",
    "        for key,value in parameters.items():\n",
    "            logger.write(\"critical\", \"{}: {}\".format(key, value))\n",
    "    \n",
    "    data = sc.read(path)\n",
    "    \n",
    "    if log_normalize:\n",
    "        sc.pp.normalize_total(data, target_sum = 1e4)\n",
    "        sc.pp.log1p(data)\n",
    "    \n",
    "    if scale_data:\n",
    "        sc.pp.scale(data)\n",
    "        \n",
    "    if convert:\n",
    "        counts = convert_data(data = data, assay = assay, label_column = label_column)\n",
    "    else:\n",
    "        counts = data.X\n",
    "        \n",
    "    if quantile_normalize:\n",
    "        quant_norm_data = convert_data(data, assay = assay)\n",
    "        counts = quantile_normalize(data.X)\n",
    "    \n",
    "    # convert string label into numeric label\n",
    "    labels = data.obs[label_column].unique().tolist()\n",
    "    labels.sort()\n",
    "    label = data.obs[label_column].apply(lambda x: labels.index(x))\n",
    "    \n",
    "    res = {\"matrix\": counts, \n",
    "           \"convert_label\": label, \n",
    "           \"original_label\": data.obs[label_column], \n",
    "           \"sort_uniq_label\": labels} \n",
    "    \n",
    "    if save:\n",
    "        file_name = \"Preprocessing_data_{}.pkl\".format(record_time())\n",
    "        with open(file_name, \"wb\") as output:\n",
    "            pickle.dump(res, output)\n",
    "    \n",
    "    if logger != \"None\": \n",
    "        logger.write(\"info\", \"finish pre processing\")\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "9cf4f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_rank_weight(x):\n",
    "    \"\"\"\n",
    "    A function to reverse rank weight to makre sure the greatest ranking representing the most importanceã€‚\n",
    "    \"\"\"\n",
    "    assert isinstance(x, dict), \"x should be dictionary\"\n",
    "    unique_values = list(set(x.values()))\n",
    "    unique_values.sort()\n",
    "    half_length = int(len(unique_values) / 2)\n",
    "    \n",
    "    d = dict()\n",
    "    \n",
    "    for key, value in x.items():\n",
    "        if value > unique_values[half_length]:\n",
    "            for i in range(-1, -half_length-1, -1):\n",
    "                if unique_values[i] == value:\n",
    "                    j = i + (i * -2) - 1\n",
    "                    d[key] = unique_values[j]\n",
    "        \n",
    "        else:\n",
    "            for i in range(half_length+1):\n",
    "                if unique_values[i] == value:\n",
    "                    j = i + 1\n",
    "                    d[key] = unique_values[-j]\n",
    "    return d    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "85f027d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(data, \n",
    "                      label_column,\n",
    "                      filename,\n",
    "                      logger = None,\n",
    "                      mode = \"ensemble\",\n",
    "                      rank_method = \"min\",\n",
    "                      merge_rank_method = \"median\",\n",
    "                      random_foreast_threshold = None,\n",
    "                      #SVM_threshold = None, \n",
    "                      variance_threshold = \"zero\",\n",
    "                      mutual_info = True, \n",
    "                      chi_square_test = False,\n",
    "                      F_test = False,\n",
    "                      model = \"random_foreast\",\n",
    "                      n_estimators = 100,\n",
    "                      random_state = 10,\n",
    "                      multi_class = \"multinomial\",\n",
    "                      kernel = \"linear\",\n",
    "                      decision_function_shape = \"ovo\",\n",
    "                      n_features_to_select = None,\n",
    "                      step = 100,\n",
    "                      save = True):\n",
    "    \"\"\"\n",
    "    A function to do feature seletion based on filtering, embedding and wrapping method respectively or combing those methods together.\n",
    "    \n",
    "    Parameters:\n",
    "    data: A pandas data frame object.\n",
    "    label_column: The name of cell type column in the data.\n",
    "    logger: A log_file object to write log information into disk. Default is None. \n",
    "    random_foreast_threshold. A float or int value to set the cutoff (feature_importance_) by random foreast model-basedd embedding feature selection.It needs to be specified when model is set in 'random_foreast'. Default is `1 / the number of all features`\n",
    "    variance_threshold: A string to decide which variance cutoff is used to filter out features.\"zero\" or \"median\" could be selected. \n",
    "    mutual_info: Bool value decide whether a mutual information method is employed to filtering out features further.\n",
    "    chi_sqaure_test: A Bool value decide whether a chi square test method is employed to filtering out features further.\n",
    "    F_test: Bool value decide whether a F test method is employed to filtering out features further.\n",
    "    model: String to decide which model is used by embedding-based feature selection. \"random_foreast\", \"logistic\" and \"svm\" could be selected.\n",
    "    n_estimators: The number of trees in the forest.\n",
    "    random_state: Controls both the randomness of the bootstrapping of the samples used when building trees (if ``bootstrap=True``) and the sampling of thefeatures to consider when looking for the best split at each node.\n",
    "    kernel: Specifies the kernel type to be used in the support vector machine algorithm.\n",
    "    decision_function_shape: Whether to return a one-vs-rest ('ovr') decision function of shape (n_samples, n_classes) as all other classifiers, or the originalone-vs-one ('ovo') decision function of libsvm which has shape.(n_samples, n_classes * (n_classes - 1) / 2)\n",
    "    n_featurs_to_selct: int or float, default=None.The number of features to select. If `None`, half of the features are selected. If integer, the parameter is the absolute number of features to select. If float between 0 and 1, it is the fraction of features to select.\n",
    "    step:int or float, default=1. If greater than or equal to 1, then ``step`` corresponds to the (integer) number of features to remove at each iteration. If within (0.0, 1.0), then ``step`` corresponds to the percentage (rounded down) of features to remove at each iteration.\n",
    "    save: Bool value to decide whether write the pre-processed data into the disk.\n",
    "    \"\"\"\n",
    "    message = \"{} {}\".format(record_time(), \"start feature selection\")\n",
    "    print(message)\n",
    "    \n",
    "    if logger != None:\n",
    "        logger.write(\"info\", remove_time(message))\n",
    "    \n",
    "        parameters = {\n",
    "                     \"label_column\": label_column,\n",
    "                     \"file_name\": filename,\n",
    "                     \"mode\": mode,\n",
    "                     \"rank_method\": rank_method,\n",
    "                     \"random_foreast_threshold\": random_foreast_threshold,\n",
    "                     \"variance_threshold\": variance_threshold,\n",
    "                     \"mutual_info\": mutual_info,\n",
    "                     \"chi_square_test\": chi_square_test,\n",
    "                      \"F_test\": F_test,\n",
    "                      \"model\" : model,\n",
    "                      \"n_estimators\": n_estimators,\n",
    "                      \"random_state\": random_state,\n",
    "                      \"multi_class\": multi_class,\n",
    "                      \"kernel\": kernel,\n",
    "                      \"decision_function_shape\": decision_function_shape,\n",
    "                      \"n_features_to_select\": n_features_to_select,\n",
    "                      \"step\" : step,\n",
    "                      \"save\" : save\n",
    "                     }\n",
    "\n",
    "        for key, value in parameters.items():\n",
    "            logger.write(\"critical\", \"{}: {}\".format(key, value))\n",
    "    \n",
    "    # step 1 - convert category label into numeric label\n",
    "    message = \"{} step 1 - converting categoric label into numeric label\".format(record_time())\n",
    "    print(message)\n",
    "    if logger != None:\n",
    "        logger.write(\"info\", remove_time(message))\n",
    "    \n",
    "    if isinstance(data[label_column].tolist()[0], str): \n",
    "        le = LabelEncoder().fit(data[label_column])\n",
    "        label = le.transform(data[label_column])\n",
    "        data[label_column] = label\n",
    "        \n",
    "    X = data.iloc[:, 1:-1]\n",
    "    y = data.iloc[:, -1]\n",
    "    \n",
    "    labels = list(le.classes_)\n",
    "    all_features = list(data.iloc[:, 1:-1].columns)\n",
    "    \n",
    "    # step 2 - do feature selection\n",
    "    message = \"{} step 2 - do feature selection\".format(record_time())\n",
    "    print(message)\n",
    "    if logger != None:\n",
    "        logger.write(\"info\", remove_time(message))\n",
    "    \n",
    "    message = \"{} ======== filtering based selection ========\".format(record_time())\n",
    "    print(message)\n",
    "    if logger != None:\n",
    "        logger.write(\"info\", remove_time(message))\n",
    "    \n",
    "        \n",
    "    # filtering-based feature selection\n",
    "    # filter out by variance \n",
    "    if variance_threshold == \"zero\":\n",
    "        var_selector = VarianceThreshold()\n",
    "        X_var = var_selector.fit_transform(X)\n",
    "        if mode == \"ensemble\":\n",
    "            retained_features_ranking_by_variance = dict([*zip(var_selector.feature_names_in_, rankdata(var_selector.variances_, method = rank_method))])\n",
    "            \n",
    "            message = \"* {} {}\".format(record_time(), \"feature ranking based on variance of each feature\")\n",
    "            print(message)\n",
    "            if logger != None:\n",
    "                logger.write(\"info\", remove_time(message))\n",
    "        else:\n",
    "            retained_features_by_filter = X.columns[var_selector.get_support()]\n",
    "            \n",
    "            message = \"* {} {} features remained after filter out features with 0 variance\".format(record_time(), X_var.shape[1])\n",
    "            print(message)\n",
    "            if logger != None:\n",
    "                logger.write(\"info\", remove_time(message))\n",
    "        \n",
    "\n",
    "    elif variance_threshold == \"median\":\n",
    "        var_selector = VarianceThreshold(np.median(np.var(np.array(X), axis = 0)))\n",
    "        X_var = var_selector.fit_transform(X)\n",
    "        if mode == \"ensemble\":\n",
    "            retained_features_ranking_by_variance = dict([*zip(var_selector.feature_names_in_, rankdata(var_selector.variances_, method = rank_method))])\n",
    "            \n",
    "            message = \"* {} {}\".format(record_time(), \"feature ranking based on variance of each feature\")\n",
    "            print(message)\n",
    "            if logger != None:\n",
    "                logger.write(\"info\", remove_time(message))\n",
    "        else:\n",
    "            retained_features_ranking_by_variance_filter = X.columns[var_selector.get_support()]\n",
    "            \n",
    "            message = \"* {} {} features remained after filter out features below median variance of all features\".format(record_time(), X_var.shape[1])\n",
    "            print(message)\n",
    "            if logger != None:\n",
    "                logger.write(\"info\", remove_time(message))\n",
    "\n",
    "    # filter by chi sqaure test\n",
    "    if chi_square_test and F_test == False:\n",
    "        if mode == \"ensemble\":\n",
    "            chivalue, pvalues_chi = chi2(X, y)\n",
    "            # when pvalue is NaN, it will be replaced with 1\n",
    "            pvalues_chi = [1 if np.isnan( _ ) else _ for _ in pvalues_chi]\n",
    "            retained_features_ranking_by_correlation = dict([*zip(all_features, rankdata(pvalues_chi, method = rank_method))])\n",
    "            # reverse the ranking weight to make sure the greatest weight representing the most importance\n",
    "            retained_features_ranking_by_correlation = reverse_rank_weight(retained_features_ranking_by_correlation)\n",
    "            \n",
    "            message = \"* {} {}\".format(record_time(), \"feature ranking based on p value calculated by chi square test to check the correlation between feature and lable\")\n",
    "            print(message)\n",
    "            if logger != None:\n",
    "                logger.write(\"info\", remove_time(message))\n",
    "        \n",
    "        else:\n",
    "            chivalue, pvalues_chi = chi2(X_var, y)\n",
    "            k = chivalue.shape[0] - (pvalues_chi > 0.05).sum()\n",
    "            selector = SelectKBest(chi2, k = k)\n",
    "            X_fschi = selector.fit_transform(X_var, y)\n",
    "            retained_features_by_filter = retained_features_by_filter[selector.get_support()]\n",
    "            \n",
    "            message = \"** {} {} features remained after further chi sqaure test filtering\".format(record_time(), X_fschi.shape[1])\n",
    "            print(message)\n",
    "            if logger != None:\n",
    "                logger.write(\"info\", remove_time(message))\n",
    "\n",
    "    # filter by F test\n",
    "    if F_test and chi_square_test == False:\n",
    "        if mode == \"ensemble\":\n",
    "            F, pvalues_f = f_classif(np.array(X), y)\n",
    "            # when pvalue is NaN, it will be replaced with 1\n",
    "            pvalues_f = [1 if np.isnan( _ ) else _ for _ in pvalues_f]\n",
    "            retained_features_ranking_by_correlation = dict([*zip(all_features, rankdata(pvalues_f, method =  rank_method))])\n",
    "            # reverse the ranking weight to make sure the greatest weight representing the most importance\n",
    "            retained_features_ranking_by_correlation = reverse_rank_weight(retained_features_ranking_by_correlation)\n",
    "            \n",
    "            message = \"* {} {}\".format(record_time(), \"feature ranking based on p value calculated by F test to check the correlation between feature and lable\")\n",
    "            print(message)\n",
    "            if logger != None:\n",
    "                logger.write(\"info\", remove_time(message))\n",
    "        \n",
    "        else:\n",
    "            F, pvalues_f = f_classif(X_var, y)\n",
    "            k = F.shape[0] - (pvalues_f > 0.05).sum()\n",
    "            selector = SelectKBest(f_classif, k = k)\n",
    "            X_fsF = selector.fit_transform(X_var, y)\n",
    "            retained_features_by_filter = retained_features_by_filter[selector.get_support()]\n",
    "            \n",
    "            message = \"** {} {} features remained after further chi sqaure test filtering\".format(record_time(), X_fsF.shape[1])\n",
    "            print(message)\n",
    "            if logger != None:\n",
    "                logger.write(\"info\", remove_time(message))\n",
    "\n",
    "    # filter by mutual infomation\n",
    "    if (F_test == False and chi_square_test == False) and mutual_info:\n",
    "        if mode == \"ensemble\":\n",
    "            res = mutual_info_classif(X, y)\n",
    "            retained_features_ranking_by_correlation = dict([*zip(all_features, rankdata(res, method = rank_method))])\n",
    "            \n",
    "            message = \"* {} {}\".format(record_time(), \"feature ranking based on p value calculated by chi square test to check the correlation between feature and lable\")\n",
    "            print(message)\n",
    "            if logger != None:\n",
    "                logger.write(\"info\", remove_time(message))\n",
    "        \n",
    "        else:\n",
    "            res = mutual_info_classif(X_var, y)\n",
    "            k = res.shape[0] - sum(res <= 0)\n",
    "            selector = SelectKBest(mutual_info_classif, k = k)\n",
    "            X_fsmic = selector.fit_transform(X_var, y)\n",
    "            retained_features_by_filter = retained_features_by_filter[selector.get_support()]\n",
    "            \n",
    "            message = \"** {} {} features remained after further mutual information filtering\".format(record_time(), X_fsmic.shape[1])\n",
    "            print(message)\n",
    "            if logger != None:\n",
    "                logger.write(\"info\", remove_time(message))\n",
    "\n",
    "    message = \"{} ======== embedding based selection ========\".format(record_time())\n",
    "    print(message)\n",
    "    if logger != None:\n",
    "        logger.write(\"info\", remove_time(message))\n",
    "\n",
    "    # embedding-based on feature selection\n",
    "    # select by random foreast model\n",
    "    if model == \"random_foreast\":\n",
    "        RFC_ = RandomForestClassifier(n_estimators = n_estimators, random_state = random_state)\n",
    "        # when random_foreast_threshold is None, \n",
    "        #  `1 / number of features` will be used as threshold\n",
    "        if random_foreast_threshold == None:\n",
    "            random_foreast_threshold = 1 / X.shape[1] \n",
    "        RFC_embedding_selector = SelectFromModel(RFC_, threshold = random_foreast_threshold)\n",
    "        X_RFC_embedding = RFC_embedding_selector.fit_transform(X, y)\n",
    "        \n",
    "        if mode == \"ensemble\":\n",
    "            retained_features_ranking_by_embedding = dict([*zip(RFC_embedding_selector.feature_names_in_ , rankdata(RFC_embedding_selector.estimator_.feature_importances_, method = rank_method))])\n",
    "        \n",
    "            message = \"* {} {}\".format(record_time(), \"feature ranking based on feature importance calcualted by random foreast model\")\n",
    "            print(message)\n",
    "            if logger != None:\n",
    "                logger.write(\"info\", remove_time(message))\n",
    "        \n",
    "        else:\n",
    "            retained_features_by_embedding = RFC_embedding_selector.get_feature_names_out()\n",
    "            \n",
    "            message = \"* {} {} features remained after random foreast based embedding filtering\".format(record_time(), X_RFC_embedding.shape[1])\n",
    "            print(message)\n",
    "            if logger != None:\n",
    "                logger.write(\"info\", remove_time(message))\n",
    "\n",
    "    # select by logistic regression model\n",
    "    elif model == \"logistic\":\n",
    "        logistic_ = LogisticRegression(multi_class = \"multinomial\", random_state = random_state, max_iter=200)\n",
    "        log_embedding_selector = SelectFromModel(logistic_, norm_order = 1)\n",
    "        X_log_embedding = log_embedding_selector.fit_transform(X, y)\n",
    "        if mode == \"ensemble\":\n",
    "            retained_features_ranking_by_embedding = dict([*zip(log_embedding_selector.feature_names_in_, rankdata(log_embedding_selector.estimator_.coef_, method = rank_method))])\n",
    "            \n",
    "            message = \"* {} {}\".format(record_time(), \"feature ranking based on coefficient calcualted by logistic model\")\n",
    "            print(message)\n",
    "            if logger != None:\n",
    "                logger.write(\"info\", remove_time(message))\n",
    "        \n",
    "        else:\n",
    "            retained_features_by_embedding = log_embedding_selector.get_feature_names_out()\n",
    "            \n",
    "            message = \"* {} {} features remained after logistic regression based embedding filtering\".format(record_time(), X_log_embedding.shape[1])\n",
    "            print(message)\n",
    "            if logger != None:\n",
    "                logger.write(\"info\", remove_time(message))\n",
    "\n",
    "    # select by SVM model\n",
    "    elif model  == \"svm\":\n",
    "        SVC_ = SVC(decision_function_shape = decision_function_shape, kernel = kernel)\n",
    "        SVC_embedding_selector = SelectFromModel(SVC_, norm_order = 1)\n",
    "        X_SVC_embedding = SVC_embedding_selector.fit_transform(np.array(X), y)\n",
    "        if mode == \"ensemble\":\n",
    "            pass\n",
    "        else:\n",
    "            retained_features_by_embedding = X.columns[SVC_embedding_selector.get_support()]\n",
    "            message = \"* {} {} features remained after svm based embedding filtering\".format(record_time(), X_SVC_embedding.shape[1])\n",
    "            print(message)\n",
    "            if logger != None:\n",
    "                logger.write(\"info\", remove_time(message))\n",
    "\n",
    "    # selected by wrapping method\n",
    "    message = \"{} ======== wrapping based selection ========\".format(record_time())\n",
    "    print(message)\n",
    "    if logger != None:\n",
    "        logger.write(\"info\", remove_time(message))\n",
    "    \n",
    "    if n_features_to_select == None:\n",
    "        # when features to select is None,\n",
    "        # 50% of all features will be used as threshold\n",
    "        n_features_to_select = int(X.shape[1] * 0.5)\n",
    "\n",
    "    if model == \"random_foreast\":\n",
    "        RFC_ = RandomForestClassifier(n_estimators = n_estimators, random_state = random_state)\n",
    "        RFC_wrapping_selector = RFE(RFC_, n_features_to_select = n_features_to_select, step = step)\n",
    "        X_RFC_wrapping = RFC_wrapping_selector.fit_transform(X, y)\n",
    "        if mode  == \"ensemble\":\n",
    "            retained_features_ranking_by_wrapping = dict([*zip(RFC_wrapping_selector.feature_names_in_ , RFC_wrapping_selector.ranking_)])\n",
    "            retained_features_ranking_by_wrapping = reverse_rank_weight(retained_features_ranking_by_wrapping)\n",
    "            \n",
    "            message = \"* {} {}\".format(record_time(), \"feature ranking based on ranking provided RFE - random foreast model\")\n",
    "            print(message)\n",
    "            if logger != None:\n",
    "                logger.write(\"info\", remove_time(message))\n",
    "        \n",
    "        else:\n",
    "            retained_features_by_wrapping = X.columns[RFC_wrapping_selector.support_]\n",
    "            \n",
    "            message = \"* {} {} features remained after RFE - random foreast based wrapping filtering\".format(record_time(), X_RFC_wrapping.shape[1])\n",
    "            print(message)\n",
    "            if logger != None:\n",
    "                logger.write(\"info\", remove_time(message))\n",
    "\n",
    "    elif model == \"logistic\":\n",
    "        logistic_ = LogisticRegression(multi_class = \"multinomial\", random_state = random_state, max_iter=200)\n",
    "        log_wrapping_selector = RFE(logistic_, n_features_to_select = n_features_to_select, step = step)\n",
    "        X_log_wrapping = log_wrapping_selector.fit_transform(X, y)\n",
    "        if mode  == \"ensemble\":\n",
    "            retained_features_ranking_by_wrapping = dict([*zip(log_wrapping_selector.feature_names_in_, log_wrapping_selector.ranking_)])\n",
    "            retained_features_ranking_by_wrapping = reverse_rank_weight(retained_features_ranking_by_wrapping)\n",
    "            \n",
    "            message = \"* {} {}\".format(record_time(), \"feature ranking based on ranking provided RFE - logistic model\")\n",
    "            print(message)\n",
    "            if logger != None:\n",
    "                logger.write(\"info\", remove_time(message))\n",
    "                \n",
    "        else:\n",
    "            retained_features_by_wrapping = X.columns[log_wrapping_selector.support_]\n",
    "            message = \"* {} {} features remained after RFE - logistic regression based wrapping filtering\".format(record_time(), X_log_wrapping.shape[1])\n",
    "            print(message)\n",
    "            if logger != None:\n",
    "                logger.write(\"info\", remove_time(message))\n",
    "\n",
    "    elif model == \"svm\":\n",
    "        SVC_ = SVC(decision_function_shape = decision_function_shape, kernel = kernel)\n",
    "        SVC_wrapping_selector = RFE(SVC_, n_features_to_select = n_features_to_select, step = step)\n",
    "        X_SVC_wrapping = SVC_wrapping_selector.fit_transform(np.array(X), y)\n",
    "        if mode  == \"ensemble\":\n",
    "            retained_features_ranking_by_wrapping = dict([*zip(SVC_wrapping_selector.feature_names_in_, SVC_wrapping_selector.ranking_)])\n",
    "            retained_features_ranking_by_wrapping = reverse_rank_weight(retained_features_ranking_by_wrapping)\n",
    "        \n",
    "            message = \"* {} {}\".format(record_time(), \"feature ranking based on ranking provided RFE - SVM model\")\n",
    "            print(message)\n",
    "            if logger != None:\n",
    "                logger.write(\"info\", remove_time(message))\n",
    "                \n",
    "        else:\n",
    "            retained_features_by_wrapping = X.columns[SVC_wrapping_selector.support_]\n",
    "            message = \"* {} {} features remained after RFE - svm based wrapping filtering\".format(record_time(), X_SVC_wrapping.shape[1])\n",
    "            print(message)\n",
    "            if logger != None:\n",
    "                logger.write(\"info\", remove_time(message))\n",
    "\n",
    "    message = \"{} ======== final feature selection ========\".format(record_time())\n",
    "    print(message)\n",
    "    if logger != None:\n",
    "        logger.write(\"info\", remove_time(message))\n",
    "        \n",
    "    \n",
    "    if mode == \"ensemble\":\n",
    "        rank_ls = list()\n",
    "        \n",
    "        for feature in all_features:\n",
    "            ranks = [retained_features_ranking_by_variance[feature], retained_features_ranking_by_correlation[feature], retained_features_ranking_by_embedding[feature], retained_features_ranking_by_wrapping[feature]]\n",
    "            if merge_rank_method == \"max\":\n",
    "                rank_ls.append((feature, max(ranks)))\n",
    "            elif merge_rank_method == \"median\":\n",
    "                rank_ls.append((feature, np.median(np.array(ranks))))\n",
    "            elif merge_rank_method == \"mean\":\n",
    "                rank_ls.append((feature, np.mean(np.array(ranks))))\n",
    "            elif merge_rank_method == \"geom.mean\":\n",
    "                rank_ls.append((feature, gmean(np.array(ranks))))\n",
    "                \n",
    "        \n",
    "        rank_ls = sorted(rank_ls, key = lambda x: -x[1])\n",
    "        rank_ls = [ _[0] for _ in rank_ls[0:n_features_to_select] ]\n",
    "        \n",
    "        output = {\"retained_features_ranking_by_variance\": retained_features_ranking_by_variance,\n",
    "                  \"retained_features_ranking_by_correlation\": retained_features_ranking_by_correlation,\n",
    "                  \"retained_features_ranking_by_embedding\": retained_features_ranking_by_embedding,\n",
    "                  \"retained_features_ranking_by_wrapping\": retained_features_ranking_by_wrapping,\n",
    "                  \"final_feature_selection\": rank_ls}\n",
    "        \n",
    "        message = \"* {} {} features were seleted by feature ranking based on variance, correlation, embedding ({}) and wrapping (RFE-{}) methods\".format(record_time(), n_features_to_select, model, model)\n",
    "        print(message)\n",
    "        if logger != None:\n",
    "            logger.write(\"info\", remove_time(message))\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        retained_features_by_filter = set(retained_features_by_filter)\n",
    "        retained_features_by_embedding = set(retained_features_by_embedding)\n",
    "        retained_features_by_wrapping = set(retained_features_by_wrapping)\n",
    "\n",
    "        if mode == \"intersection\":\n",
    "            final_feture_selection = reduce(lambda x,y: x.intersection(y), [retained_features_by_embedding, retained_features_by_filter, retained_features_by_wrapping])\n",
    "\n",
    "        if mode == \"union\":\n",
    "             final_feture_selection = reduce(lambda x,y: x.union(y), [retained_features_by_embedding, retained_features_by_filter, retained_features_by_wrapping])\n",
    "    \n",
    "        message = \"* {} {} features remained after intersecting the key features found by filtering, embedding and wrapping-based feature selection methods\".format(record_time(), len(final_feture_selection))\n",
    "        print(message)\n",
    "        if logger != None:\n",
    "            logger.write(\"info\", remove_time(message))\n",
    "\n",
    "        output = {\"retained_features_by_filtering\": retained_features_by_filter,\n",
    "                 \"retained_features_by_embedding\": retained_features_by_embedding,\n",
    "                 \"retained_features_by_wrapping\": retained_features_by_wrapping,\n",
    "                 \"final_feature_selection\": final_feture_selection}\n",
    "    \n",
    "    if save == True:\n",
    "        filename = filename + \"_\" + model + \"_\" + \"feature_selection\" + \"_\" + record_time() + \".pkl\"\n",
    "        with open(filename, \"wb\") as file:\n",
    "            pickle.dump(output, file)\n",
    "    \n",
    "    message = \"{} finish feature selection\".format(record_time())\n",
    "    print(message)\n",
    "    if logger != None:\n",
    "        logger.write(\"info\", remove_time(message))\n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "38b08696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(data,\n",
    "                   label_column,\n",
    "                   features,\n",
    "                   model,\n",
    "                   filename = None,\n",
    "                   logger = None,\n",
    "                   test_size = 0.3,\n",
    "                   random_state = 10,\n",
    "                   cv = 10,\n",
    "                   save = True,\n",
    "                   njobs = 30\n",
    "                  ):\n",
    "    \n",
    "    \"\"\"\n",
    "    A function to do model training.\n",
    "    \n",
    "    Parameters:\n",
    "    d data: A pandas data frame object.\n",
    "    label_column: The name of cell type column in the data.\n",
    "    features: Feaures should be kept for model training in the data.\n",
    "    model: Algorithm to train model. \"random_foreast\", \"svm\" or \"logistic\" could be selected. \n",
    "    test_size: \n",
    "    logger: A log_file object to write log information into disk. Default is None. \n",
    "    test_size: Percentage of data remained for testing model.\n",
    "    random_state:\n",
    "    cv: The number of cross validation for grid serach.\n",
    "    save: Bool value to decide whether the result will be written into disk. Default is True.\n",
    "    \"\"\"\n",
    "    \n",
    "    message = \"{} start model training\".format(record_time())\n",
    "    print(message)\n",
    "    print(\"{} model traning based on {} algorithm\".format(record_time(), model))\n",
    "    \n",
    "    if logger != None:\n",
    "        logger.write(\"info\", remove_time(message))\n",
    "        \n",
    "        parameters = {\"label_column\": label_column,\n",
    "                     \"features\": features,\n",
    "                     \"model\": model,\n",
    "                     \"test_size\": test_size,\n",
    "                     \"random_state\": random_state,\n",
    "                     \"cv\": cv,\n",
    "                     \"save\": save}\n",
    "        \n",
    "        for key,value in parameters.items():\n",
    "            logger.write(\"critical\", \"{}: {}\".format(key, value))\n",
    "        \n",
    "        logger.write(\"info\", \"model traning based on {} algorithm\".format(model))\n",
    "    \n",
    "    X = data.loc[:, data.columns != label_column]\n",
    "    y = data.loc[:, data.columns == label_column]\n",
    "    \n",
    "    le = LabelEncoder().fit(y)\n",
    "    y_trans = le.transform(y)\n",
    "    \n",
    "    # only keep informative features\n",
    "    X = data.loc[:, data.columns.isin(features)] \n",
    "    \n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y_trans, test_size = test_size)\n",
    "    \n",
    "    if model == \"random_foreast\":\n",
    "        \n",
    "        parameters = {\"n_estimators\" : np.arange(10, 101, 10),\n",
    "                     \"criterion\" : [\"gini\", \"entropy\"],\n",
    "                     # \"max_depth\": np.linspace(10, 50, 5),\n",
    "                     \"max_features\": np.linspace(0.2, 1, 5)}\n",
    "                     #\"min_samples_leaf\": np.arange(10, 300, 20),\n",
    "                     #\"min_samples_split\": np.arange(2, 100, 10)}\n",
    "        \n",
    "        message = \"{} grid search below paramters getting the best model\".format(record_time())\n",
    "        print(message)\n",
    "        \n",
    "        if logger != None:\n",
    "            logger.write(\"info\", remove_time(message))\n",
    "        \n",
    "        for key,value in parameters.items():\n",
    "            print(\"* {}: {}\".format(key, value))\n",
    "            if logger != None:\n",
    "                logger.write(\"critical\", \"{}: {}\".format(key, value))\n",
    "           \n",
    "        \n",
    "        RFC_ = RandomForestClassifier(random_state = random_state)\n",
    "        GS = GridSearchCV(RFC_, parameters, cv = cv, njobs = njobs)\n",
    "        GS.fit(Xtrain, Ytrain)\n",
    "        best_parameters = GS.best_params_\n",
    "        best_core = GS.best_score_\n",
    "        score_on_test_data = GS.score(Xtest, Ytest)\n",
    "    \n",
    "    elif model == \"logistic\":\n",
    "        \n",
    "        parameters = {\"penalty\": [\"l1\", \"l2\"],\n",
    "                      \"C\": np.linspace(0,1,5),\n",
    "                      \"multi_class\": [\"ovr\", \"multinomial\"]\n",
    "                     }\n",
    "        \n",
    "        message = \"{} grid search below paramters getting the best model\".format(record_time())\n",
    "        print(message)\n",
    "        \n",
    "        if logger != None:\n",
    "            logger.write(\"info\", remove_time(message))\n",
    "        \n",
    "        for key,value in parameters.items():\n",
    "            print(\"* {}: {}\".format(key, value))\n",
    "            if logger != None:\n",
    "                logger.write(\"critical\", \"{}: {}\".format(key, value))\n",
    "        \n",
    "        logistic_ = LogisticRegression()\n",
    "        GS = GridSearchCV(logistic_, parameters, cv = cv, njobs = njobs)\n",
    "        GS.fit(Xtrain, Ytrain)\n",
    "        best_parameters = GS.best_params_\n",
    "        best_core = GS.best_score_\n",
    "        score_on_test_data = GS.score(Xtest, Ytest)\n",
    "    \n",
    "    elif model == \"svm\":\n",
    "        # scale data for SVM\n",
    "        Xtrain = StandardScaler().fit_transform(np.array(Xtrain))\n",
    "        Xtest = StandardScaler().fit_transform(np.array(Xtest))\n",
    "        \n",
    "        parameters = {\"C\": np.linspace(0.01,30,50),\n",
    "                     \"kernel\": [\"rbf\", \"poly\", \"sigmoid\", \"linear\"],\n",
    "                     \"gamma\": [\"auto\", \"scale\"],\n",
    "                     \"coef0\": np.linspace(0,5,10)}\n",
    "        \n",
    "        message = \"{} grid search below paramters getting the best model\".format(record_time())\n",
    "        print(message)\n",
    "        \n",
    "        if logger != None:\n",
    "            logger.write(\"info\", remove_time(message))\n",
    "        \n",
    "        for key,value in parameters.items():\n",
    "            print(\"* {}: {}\".format(key, value))\n",
    "            if logger != None:\n",
    "                logger.write(\"critical\", \"{}: {}\".format(key, value))\n",
    "        \n",
    "        SVM_ = SVC()\n",
    "        GS = GridSearchCV(SVM_, parameters, cv = cv, njobs = njobs)\n",
    "        GS.fit(Xtrain, Ytrain)\n",
    "        best_parameters = GS.best_params_\n",
    "        best_core = GS.best_score_\n",
    "        score_on_test_data = GS.score(Xtest, Ytest)\n",
    "        \n",
    "    \n",
    "    output = {\"model\": GS,\n",
    "              \"best_score\": best_core,\n",
    "              \"best_parameters\": best_parameters,\n",
    "              \"score_on_test_data\": score_on_test_data,\n",
    "             \"features_used_for_training\": features}\n",
    "    \n",
    "    if logger != None:\n",
    "        for key, value in output.items():\n",
    "            logger.write(\"info\", \"{}: {}\".format(key, value))\n",
    "    \n",
    "    if save == True:\n",
    "        if filename != None:\n",
    "            filename = model + \"_\" + \"training_model\" + \"_\" + record_time() + \".pkl\"\n",
    "        else:\n",
    "            filename = filename + \"_\" + model + \"_\" + \"training_model\" + \"_\" + record_time() + \".pkl\"\n",
    "        \n",
    "        with open(filename, \"wb\") as file:\n",
    "            pickle.dump(output, file)\n",
    "    \n",
    "    message = \"{} finish model training\".format(record_time())\n",
    "    print(message)\n",
    "    if logger != None:\n",
    "        logger.write(\"info\", remove_time(message))\n",
    "    \n",
    "    return(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6743ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle_file(file_name, wk_dir = os.getcwd(), recursive = True):\n",
    "    os.chdir(wk_dir)\n",
    "    file_path = list_files(os.getcwd(), pattern = file_name, recursive = recursive)[0]\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        pickle_file = pickle.load(f)\n",
    "        \n",
    "    return pickle_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "955b27ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a log_file object for downstream recording log information\n",
    "#log_file.clear()\n",
    "my_logger = log_file(\"simulation_test3\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a36edae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fengtang/miniconda3/lib/python3.11/site-packages/anndata/__init__.py:51: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = pre_processing(\"./cellhint_demo_folder/cellhint_demo_folder/Spleen.h5ad\", \n",
    "                      assay = \".X\", \n",
    "                      logger = my_logger,\n",
    "                      label_column = \"cell_type\",\n",
    "                      convert = True,\n",
    "                      scale_data = False,\n",
    "                      save = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "b223486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = random.choices(range(74369), k = 19999)\n",
    "ls.sort()\n",
    "ls.append(74369)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f885d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [\"ensemble\", \"intersection\"]:\n",
    "    \n",
    "    test = feature_selection(data=data[\"matrix\"].iloc[0:20000, ls], \n",
    "                  label_column=\"cell_type\", \n",
    "                  filename = i,\n",
    "                  variance_threshold=\"zero\",\n",
    "                  mode = i,\n",
    "                  model=\"random_foreast\",\n",
    "                  merge_rank_method=\"geom.mean\",\n",
    "                  chi_square_test = False,\n",
    "                  F_test = True,\n",
    "                  mutual_info = False,\n",
    "                  n_features_to_select = 3000,\n",
    "                  logger = my_logger\n",
    "                 )\n",
    "    \n",
    "    test2 = model_training(data = data[\"matrix\"].iloc[0:20000, ls],\n",
    "                      label_column = \"cell_type\",\n",
    "                      filename = i,\n",
    "                      features = test[\"final_feature_selection\"],\n",
    "                      model = \"random_foreast\",\n",
    "                      logger = my_logger,\n",
    "                      cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "df43c9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-08 17:16:49 start feature selection\n",
      "2024-01-08 17:16:49 step 1 - converting categoric label into numeric label\n",
      "2024-01-08 17:16:49 step 2 - do feature selection\n",
      "2024-01-08 17:16:49 ======== filtering based selection ========\n",
      "* 2024-01-08 17:16:49 feature ranking based on variance of each feature\n",
      "* 2024-01-08 17:16:49 feature ranking based on p value calculated by F test to check the correlation between feature and lable\n",
      "2024-01-08 17:16:49 ======== embedding based selection ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fengtang/miniconda3/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [  1   2   6   7   9  14  16  17  19  21  26  29  30  38  47  49  61  63\n",
      "  69  71  72  73  74  75  76  77  81  82  89  93  95  96  98  99 100 101\n",
      " 102 103 104 105 107 109 110 111 113 114 115 116 117 118 119 120 121 122\n",
      " 123 124 132 134 142 151 164 165 181 183 184 187 190 192 193 196 202 207\n",
      " 209 210 216 217 222 224 226 229 230 231 235 236 237 238 239 242 243 244\n",
      " 247 248 249 251 252 253 254 255 256 257 260 265 266 269 272 273 274 277\n",
      " 278 279 280 281 282 283 284 285 286 287 288 292 293 295 297 298 299 301\n",
      " 309 314 316 318 319 320 321 322 325 327 328 329 330 331 335 337 345 346\n",
      " 349 352 353 354 356 357 358 359 360 361 362 363 364 365 366 367] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/fengtang/miniconda3/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: divide by zero encountered in divide\n",
      "  f = msb / msw\n",
      "/home/fengtang/miniconda3/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 2024-01-08 17:16:49 feature ranking based on feature importance calcualted by random foreast model\n",
      "2024-01-08 17:16:49 ======== wrapping based selection ========\n",
      "* 2024-01-08 17:16:50 feature ranking based on ranking provided RFE - random foreast model\n",
      "2024-01-08 17:16:50 ======== final feature selection ========\n",
      "* 2024-01-08 17:16:50 150 features were seleted by feature ranking based on variance, correlation, embedding (random_foreast) and wrapping (RFE-random_foreast) methods\n",
      "2024-01-08 17:16:50 finish feature selection\n"
     ]
    }
   ],
   "source": [
    "# test = feature_selection(data=data[\"matrix\"].iloc[0:20000, ls], \n",
    "#                   label_column=\"cell_type\", \n",
    "#                   filename=\"simulation_test3\",\n",
    "#                   variance_threshold=\"zero\",\n",
    "#                   mode = \"ensemble\",\n",
    "#                   model=\"random_foreast\",\n",
    "#                   merge_rank_method=\"geom.mean\",\n",
    "#                   chi_square_test = False,\n",
    "#                   F_test = True,\n",
    "#                   mutual_info = False,\n",
    "#                   n_features_to_select = 3000,\n",
    "#                   logger = my_logger\n",
    "#                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3705b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-03 09:59:59 start model training\n",
      "2024-01-03 09:59:59 model traning based on random_foreast algorithm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fengtang/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/fengtang/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-03 10:00:00 grid search below paramters getting the best model\n",
      "* n_estimators: [ 10  20  30  40  50  60  70  80  90 100]\n",
      "* criterion: ['gini', 'entropy']\n",
      "* max_features: [0.2 0.4 0.6 0.8 1. ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fengtang/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-03 10:15:20 finish model training\n"
     ]
    }
   ],
   "source": [
    "test2 = model_training(data = data[\"matrix\"].iloc[0:20000, ls],\n",
    "                      label_column = \"cell_type\",\n",
    "                      features = test[\"final_feature_selection\"],\n",
    "                      model = \"random_foreast\",\n",
    "                      logger = my_logger,\n",
    "                      cv = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
