{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4e41870",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE,VarianceThreshold,SelectKBest,chi2, SelectFromModel, f_classif, mutual_info_classif\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "065e17f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(data = None, path = None, assay = \".X\",label_column = None):\n",
    "    \"\"\"\n",
    "    A function to convert sparse matrix into pandas data frame object. \n",
    "    \n",
    "    Parameters:\n",
    "    data: a Annot Data object. If speciefied, path should be specified in 'None'.\n",
    "    path: path to h5ad file. If speciefied, data should be specified in 'None'.\n",
    "    assay: \".X\" or \"raw\" assay in Annot Data object could be specified\n",
    "    label_column: the name of cell type column in the h5ad file. \n",
    "    If specified, the cell type column will be added into output.\n",
    "    \"\"\"\n",
    "    if path != None:\n",
    "        data = sc.read(path)\n",
    "    \n",
    "    if assay == \".X\":\n",
    "        counts = pd.DataFrame.sparse.from_spmatrix(data.X)\n",
    "    else:\n",
    "        counts = pd.DataFrame.sparse.from_spmatrix(data.raw.X)\n",
    "        \n",
    "    features = data.raw.var_names.tolist()\n",
    "    index  = data.raw.obs_names.tolist()\n",
    "    \n",
    "    counts.columns = features\n",
    "    counts.index = index\n",
    "    \n",
    "    if label_column != None:\n",
    "        try:\n",
    "            labels = data.obs[label_column].tolist()\n",
    "            counts[\"cell_type\"] = labels\n",
    "        except:\n",
    "            raise ValueError(\"The length of cell type column is not consistent with matrix\")\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00060a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_normalize(data):\n",
    "    \"\"\"\n",
    "    A function to do quantile normalization.  \n",
    "    \n",
    "    \"\"\"\n",
    "    data = data.loc[:, data.columns != \"cell_type\"]\n",
    "    ranks = (data.rank(method = \"first\").stack())\n",
    "    rank_mean = (data.stack().groupby(ranks).mean())\n",
    "    # add interproblated values in between ranks\n",
    "    finer_ranks = ((rank_mean.index + 0.5).tolist() + rank_mean.index.tolist())\n",
    "    rank_mean = rank_mean.reindex(finer_ranks).sort_index().interpolate()\n",
    "    data = data.rank(method = \"average\").stack().map(rank_mean).unstack()\n",
    "    \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dce399b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_time():\n",
    "    \"\"\"\n",
    "    A function to call out the current time.\n",
    "    \"\"\"\n",
    "    current_time_stamp = time.time()\n",
    "    date_time = datetime.fromtimestamp(current_time_stamp)\n",
    "    return date_time.strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cea2ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(path, \n",
    "                   label_column,\n",
    "                   assay = \".X\", \n",
    "                   convert = True,\n",
    "                   log_normalize = True, \n",
    "                   scale_data = False, \n",
    "                   quantile_normalize = False,\n",
    "                   save = False):\n",
    "    \"\"\"\n",
    "    A function to do preliminary normalization, encoding label and convert data into pandas data frame for downstream sklearn-based\n",
    "    machine learning workflow.\n",
    "    \n",
    "    Parameters:\n",
    "    path: path to the h5ad file.\n",
    "    assay: \".X\" or \"raw\" assay in Annot Data object could be specified.Default is \".X\".\n",
    "    label_column: The name of cell type column in the h5ad file.If specified, the cell type column will be added into output.\n",
    "    convert: Bool value to decide whether convert Annot Data object into pandas data frame object.\n",
    "    log_normalize: Bool value to decide whether standard log normalization to be done.\n",
    "    scale_data: Bool value to decide whether standadlize data or not.\n",
    "    quantile_normalize: Bool value to decide whether quantile normalize data or not.\n",
    "    save: Bool value to decide whether write the pre-processed data into the disk.\n",
    "    \"\"\"\n",
    "    data = sc.read(path)\n",
    "    \n",
    "    if log_normalize:\n",
    "        sc.pp.normalize_total(data, target_sum = 1e4)\n",
    "        sc.pp.log1p(data)\n",
    "    \n",
    "    if scale_data:\n",
    "        sc.pp.scale(data)\n",
    "        \n",
    "    if convert:\n",
    "        counts = convert_data(data = data, assay = assay, label_column = label_column)\n",
    "    else:\n",
    "        counts = data.X\n",
    "        \n",
    "    if quantile_normalize:\n",
    "        quant_norm_data = convert_data(data, assay = assay)\n",
    "        counts = quantile_normalize(data.X)\n",
    "    \n",
    "    # convert string label into numeric label\n",
    "    labels = data.obs[label_column].unique().tolist()\n",
    "    labels.sort()\n",
    "    label = data.obs[label_column].apply(lambda x: labels.index(x))\n",
    "    \n",
    "    res = {\"matrix\": counts, \n",
    "           \"convert_label\": label, \n",
    "           \"original_label\": data.obs[label_column], \n",
    "           \"sort_uniq_label\": labels} \n",
    "    \n",
    "    if save:\n",
    "        file_name = \"Preprocessing_data_{}.pkl\".format(record_time())\n",
    "        with open(file_name, \"wb\") as output:\n",
    "            pickle.dump(res, output)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a36edae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fengtang/miniconda3/lib/python3.11/site-packages/anndata/__init__.py:51: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = pre_processing(\"./cellhint_demo_folder/cellhint_demo_folder/Spleen.h5ad\", assay = \".X\", \n",
    "                      label_column = \"cell_type\",\n",
    "                      convert = True,\n",
    "                      scale_data = False,\n",
    "                      save = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "85f027d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(data, \n",
    "                      label_column,\n",
    "                      random_foreast_threshold = None,\n",
    "                      SVM_threshold = None, \n",
    "                      variance_threshold = \"zero\",\n",
    "                      mutual_info = True, \n",
    "                      chi_square_test = False,\n",
    "                      F_test = False,\n",
    "                      method = \"all\",\n",
    "                      model  = [\"random_foreast\", \"logistic\", \"svm\"],\n",
    "                      n_estimators = 100,\n",
    "                      random_state = 10,\n",
    "                      kernel = \"linear\",\n",
    "                      decision_function_shape = \"ovo\"):\n",
    "    \"\"\"\n",
    "    A function to do feature seletion based on filtering, embedding and wrapping method respectively or combing those methods together.\n",
    "    \n",
    "    Parameters:\n",
    "    data: A pandas data frame object.\n",
    "    label_column: The name of cell type column in the data.\n",
    "    random_foreast_threshold. A float or int value to set the cutoff (feature_importance_) by random foreast model-basedd embedding feature selection.It needs to be specified when model is set in 'random_foreast'.\n",
    "    SVM_threshold:\n",
    "    variance_threshold: A string to decide which variance cutoff is used to filter out features.\"zero\" or \"median\" could be selected. \n",
    "    mutual_info: A Bool value decide whether a mutual information method is employed to filtering out features further.\n",
    "    chi_sqaure_test: A Bool value decide whether a chi square test method is employed to filtering out features further.\n",
    "    F_test: A Bool value decide whether a F test method is employed to filtering out features further.\n",
    "    method: A string to decide whether combining filtering, embedding and wrapping feature selection method togther or just using one of them. \"all\", \"filtering\", \"embedding\" and \"wrapping\" could be selected.\n",
    "    model: A string to decide which model is used by embedding-based feature selection. \"random_foreast\", \"logistic\" and \"svm\" could be selected.\n",
    "    n_estimators: The number of trees in the forest.\n",
    "    random_state:Controls both the randomness of the bootstrapping of the samples used when building trees (if ``bootstrap=True``) and the sampling of thefeatures to consider when looking for the best split at each node.\n",
    "    kernel: Specifies the kernel type to be used in the support vector machine algorithm.\n",
    "    decision_function_shape: Whether to return a one-vs-rest ('ovr') decision function of shape (n_samples, n_classes) as all other classifiers, or the originalone-vs-one ('ovo') decision function of libsvm which has shape.(n_samples, n_classes * (n_classes - 1) / 2)\n",
    "    \"\"\"\n",
    "    \n",
    "    # step 1 - convert category label into numeric label\n",
    "    print(\"{} step 1 - converting categoric label into numeric label\".format(record_time()))\n",
    "    le = LabelEncoder().fit(data[label_column])\n",
    "    label = le.transform(data[label_column])\n",
    "    data[label_column] = label\n",
    "        \n",
    "    X = data.iloc[:, 1:-1]\n",
    "    y = data.iloc[:, -1]\n",
    "    \n",
    "    # step 2 - do feature selection\n",
    "    print(\"{} step 2 - do feature selection\".format(record_time()))\n",
    "    print(\"{} ======== filtering based selection ========\".format(record_time()))\n",
    "    if method == \"all\":\n",
    "        \n",
    "        # filtering-based feature selection\n",
    "        # filter out by variance \n",
    "        if variance_threshold == \"zero\":\n",
    "            var_selector = VarianceThreshold()\n",
    "            X_var = var_selector.fit_transform(X)\n",
    "            retained_features_by_filter = X.columns[var_selector.get_support()]\n",
    "            print(\"* {} {} features remained after filter out features with 0 variance\".format(record_time(), X_var.shape[1]))\n",
    "        \n",
    "        elif variance_threshold == \"median\":\n",
    "            var_selector = VarianceThreshold(np.median(np.var(np.array(X), axis = 0)))\n",
    "            X_var = var_selector.fit_transform(X)\n",
    "            retained_features_by_filter = X.columns[var_selector.get_support()]\n",
    "            print(\"* {} {} features remained after filter out features below median variance of all features\".format(record_time(), X_var.shape[1]))\n",
    "            \n",
    "        # filter by chi sqaure test\n",
    "        if chi_square_test and F_test == False:\n",
    "            chivalue, pvalues_chi = chi2(X_var, y)\n",
    "            k = chivalue.shape[0] - (pvalues_chi > 0.05).sum()\n",
    "            selector = SelectKBest(chi2, k = k)\n",
    "            X_fschi = selector.fit_transform(X_var, y)\n",
    "            retained_features_by_filter = retained_features_by_filter[selector.get_support()]\n",
    "            print(\"** {} {} features remained after further chi sqaure test filtering\".format(record_time(), X_fschi.shape[1]))\n",
    "            \n",
    "        # filter by F test\n",
    "        if F_test and chi_square_test == False:\n",
    "            F, pvalues_f = f_classif(X_var, y)\n",
    "            k = F.shape[0] - (pvalues_f > 0.05).sum()\n",
    "            selector = SelectKBest(f_classif, k = k)\n",
    "            X_fsF = selector.fit_transform(X_var, y)\n",
    "            retained_features_by_filter = retained_features_by_filter[selector.get_support()]\n",
    "            print(\"** {} {} features remained after further chi sqaure test filtering\".format(record_time(), X_fsF.shape[1]))\n",
    "            \n",
    "        # filter by mutual infomation\n",
    "        if (F_test == False and chi_square_test == False) and mutual_info:\n",
    "            res = mutual_info_classif(X_var, y)\n",
    "            k = res.shape[0] - sum(res <= 0)\n",
    "            selector = SelectKBest(mutual_info_classif, k = k)\n",
    "            X_fsmic = selector.fit_transform(X_var, y)\n",
    "            retained_features_by_filter = retained_features_by_filter[selector.get_support()]\n",
    "            print(\"** {} {} features remained after further mutual information filtering\".format(record_time(), X_fsmic.shape[1]))\n",
    "        \n",
    "        print(\"{} ======== embedding based selection ========\".format(record_time()))\n",
    "        # embedding-based on feature selection\n",
    "        # select by random foreast model\n",
    "        if model == \"random_foreast\":\n",
    "            RFC_ = RandomForestClassifier(n_estimators = 100, random_state = random_state)\n",
    "            RFC_embedding_selector = SelectFromModel(RFC_, threshold = random_foreast_threshold)\n",
    "            X_RFC_embedding = RFC_embedding_selector.fit_transform(X, y)\n",
    "            retained_features_by_embedding = RFC_embedding_selector.get_feature_names_out()\n",
    "            print(\"* {} {} features remained after random foreast based embedding filtering\".format(record_time(), X_RFC_embedding.shape[1]))\n",
    "       \n",
    "        # select by logistic regression model\n",
    "        elif model == \"logistic\":\n",
    "            logistic_ = LogisticRegression(multi_class = \"multinomial\", random_state = random_state, max_iter=200)\n",
    "            log_embedding_selector = SelectFromModel(logistic_, norm_order = 1)\n",
    "            X_log_embedding = log_embedding_selector.fit_transform(X, y)\n",
    "            retained_features_by_embedding = log_embedding_selector.get_feature_names_out()\n",
    "            print(\"* {} {} features remained after logistic regression based embedding filtering\".format(record_time(), X_log_embedding.shape[1]))\n",
    "        \n",
    "        # select by SVM model\n",
    "        elif model  == \"svm\":\n",
    "            SVC_ = SVC(decision_function_shape = decision_function_shape, kernel = kernel)\n",
    "            SVC_embedding_selector = SelectFromModel(SVC_, norm_order = 1)\n",
    "            X_SVC_embedding = SVC_embedding_selector.fit_transform(np.array(X), y)\n",
    "            retained_features_by_embedding = X.columns[SVC_embedding_selector.get_support()]\n",
    "            print(\"* {} {} features remained after svm based embedding filtering\".format(record_time(), X_SVC_embedding.shape[1]))\n",
    "        \n",
    "        print(\"{} ======== final feature selection ========\".format(record_time()))\n",
    "        if isinstance(retained_features_by_filter, set) and isinstance(retained_features_by_embedding, set):\n",
    "            final_feture_selection = retained_features_by_filter.intersection(retained_features_by_embedding)\n",
    "            print(\"* {} {} features remained after intersecting the key features found by filtering and embedding-based feature selection method\".format(record_time(), len(final_feture_selection)))\n",
    "        else:\n",
    "            retained_features_by_filter = set(retained_features_by_filter)\n",
    "            retained_features_by_embedding = set(retained_features_by_embedding)\n",
    "            final_feture_selection = retained_features_by_filter.intersection(retained_features_by_embedding)\n",
    "            print(\"* {} {} features remained after intersecting the key features found by filtering and embedding-based feature selection method\".format(record_time(), len(final_feture_selection)))    \n",
    "    \n",
    "    else:\n",
    "        # filtering-based feature selection\n",
    "        if method == \"filtering\" :\n",
    "            pass\n",
    "        elif method == \"embedding\":\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "df43c9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-25 20:44:44 step 1 - converting categoric label into numeric label\n",
      "2023-12-25 20:44:44 step 2 - do feature selection\n",
      "2023-12-25 20:44:44 ======== filtering based selection ========\n",
      "* 2023-12-25 20:44:45 747 features remained after filter out features below median variance of all features\n",
      "** 2023-12-25 20:44:45 413 features remained after further chi sqaure test filtering\n",
      "2023-12-25 20:44:45 ======== embedding based selection ========\n",
      "* 2023-12-25 20:44:46 528 features remained after svm based embedding filtering\n",
      "2023-12-25 20:44:46 ======== final feature selection ========\n",
      "* 2023-12-25 20:44:46 343 features remained after intersecting the key features found by filtering and embedding-based feature selection method\n"
     ]
    }
   ],
   "source": [
    "test = feature_selection(data=data[\"matrix\"].iloc[1:1000, 72870:74370], \n",
    "                  label_column=\"cell_type\", \n",
    "                  variance_threshold=\"median\",\n",
    "                  method=\"all\", \n",
    "                  model=\"svm\",\n",
    "                  chi_square_test=True,\n",
    "                  F_test=False,\n",
    "                  mutual_info=False\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "31deb407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 1500)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
