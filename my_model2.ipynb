{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4e41870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from functools import reduce\n",
    "\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE,VarianceThreshold,SelectKBest,chi2, SelectFromModel, f_classif, mutual_info_classif\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "065e17f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(data = None, path = None, assay = \".X\",label_column = None):\n",
    "    \"\"\"\n",
    "    A function to convert sparse matrix into pandas data frame object. \n",
    "    \n",
    "    Parameters:\n",
    "    data: a Annot Data object. If speciefied, path should be specified in 'None'.\n",
    "    path: path to h5ad file. If speciefied, data should be specified in 'None'.\n",
    "    assay: \".X\" or \"raw\" assay in Annot Data object could be specified\n",
    "    label_column: the name of cell type column in the h5ad file. \n",
    "    If specified, the cell type column will be added into output.\n",
    "    \"\"\"\n",
    "    if path != None:\n",
    "        data = sc.read(path)\n",
    "    \n",
    "    if assay == \".X\":\n",
    "        counts = pd.DataFrame.sparse.from_spmatrix(data.X)\n",
    "    else:\n",
    "        counts = pd.DataFrame.sparse.from_spmatrix(data.raw.X)\n",
    "        \n",
    "    features = data.raw.var_names.tolist()\n",
    "    index  = data.raw.obs_names.tolist()\n",
    "    \n",
    "    counts.columns = features\n",
    "    counts.index = index\n",
    "    \n",
    "    if label_column != None:\n",
    "        try:\n",
    "            labels = data.obs[label_column].tolist()\n",
    "            counts[\"cell_type\"] = labels\n",
    "        except:\n",
    "            raise ValueError(\"The length of cell type column is not consistent with matrix\")\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00060a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_normalize(data):\n",
    "    \"\"\"\n",
    "    A function to do quantile normalization.  \n",
    "    \n",
    "    \"\"\"\n",
    "    data = data.loc[:, data.columns != \"cell_type\"]\n",
    "    ranks = (data.rank(method = \"first\").stack())\n",
    "    rank_mean = (data.stack().groupby(ranks).mean())\n",
    "    # add interproblated values in between ranks\n",
    "    finer_ranks = ((rank_mean.index + 0.5).tolist() + rank_mean.index.tolist())\n",
    "    rank_mean = rank_mean.reindex(finer_ranks).sort_index().interpolate()\n",
    "    data = data.rank(method = \"average\").stack().map(rank_mean).unstack()\n",
    "    \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4efc3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_time():\n",
    "    \"\"\"\n",
    "    A function to call out current time.\n",
    "    \"\"\"\n",
    "    current_second_time = datetime.now()\n",
    "    return current_second_time.strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57899021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class log_file:\n",
    "    \"\"\"\n",
    "    A class to easily write log information into different log files.\n",
    "    \n",
    "    filename: log file name \n",
    "    mode: \"a\" (append log information) or \"w\" (clean up old log information and then write new information)\n",
    "    \"\"\"\n",
    "    n_class = 0\n",
    "    def __init__(self, filename, mode):\n",
    "        self.time = record_time()\n",
    "        self.filename = filename\n",
    "        self.mode = mode\n",
    "        self.logger = None\n",
    "        self.n_object = 0\n",
    "        log_file.n_class += 1\n",
    "        \n",
    "    def set(self):\n",
    "        logger = logging.getLogger(\"my_logger\")\n",
    "        logger.setLevel(logging.INFO)\n",
    "        log_file_name = \"{}_{}.log\".format(self.filename, self.time)\n",
    "        file_handler = logging.FileHandler(log_file_name, mode = self.mode)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "        self.logger = logger\n",
    "        \n",
    "    def start(self):\n",
    "        if log_file.n_class == 1 and self.n_object == 0:\n",
    "            self.set()\n",
    "        if log_file.n_class > 1 and self.n_object == 0:\n",
    "            self.set()\n",
    "        \n",
    "    def write(self, category, content):\n",
    "        self.start()\n",
    "        self.n_object += 1\n",
    "        if category == \"info\":\n",
    "            self.logger.info(content)\n",
    "        elif category == \"error\":\n",
    "            self.logger.error(content)\n",
    "        elif category == \"warning\":\n",
    "            self.logger.warning(content)\n",
    "        elif category == \"debug\":\n",
    "            self.logger.debug(content)\n",
    "        elif category == \"critical\":\n",
    "            self.logger.critical(content)\n",
    "    \n",
    "    def mode_reset(self, mode):\n",
    "        self.mode = mode\n",
    "        \n",
    "    def clear():\n",
    "        log_file.n_class = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c745cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_time(string):\n",
    "    \"\"\"\n",
    "    A function to remove time information in the stdout message\"\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\")\n",
    "    res = re.sub(pattern, \"\", string)\n",
    "    return res.rstrip().lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "955b27ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a log_file object for downstream recording log information\n",
    "#log_file.clear()\n",
    "my_logger = log_file(\"mylogfile4\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cea2ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(path, \n",
    "                   label_column,\n",
    "                   assay = \".X\", \n",
    "                   convert = True,\n",
    "                   log_normalize = True, \n",
    "                   scale_data = False, \n",
    "                   quantile_normalize = False,\n",
    "                   save = False,\n",
    "                   logger = None):\n",
    "    \"\"\"\n",
    "    A function to do preliminary normalization, encoding label and convert data into pandas data frame for downstream sklearn-based\n",
    "    machine learning workflow.\n",
    "    \n",
    "    Parameters:\n",
    "    path: path to the h5ad file.\n",
    "    assay: \".X\" or \"raw\" assay in Annot Data object could be specified.Default is \".X\".\n",
    "    label_column: The name of cell type column in the h5ad file.If specified, the cell type column will be added into output.\n",
    "    convert: Bool value to decide whether convert Annot Data object into pandas data frame object.\n",
    "    log_normalize: Bool value to decide whether standard log normalization to be done.\n",
    "    scale_data: Bool value to decide whether standadlize data or not.\n",
    "    quantile_normalize: Bool value to decide whether quantile normalize data or not.\n",
    "    save: Bool value to decide whether write the pre-processed data into the disk.\n",
    "    logger: A log_file object to write log information into disk. Default is None. \n",
    "    \"\"\"\n",
    "    if logger != \"None\":\n",
    "        logger.write(\"info\", \"start pre processing\")\n",
    "        logger.write(\"critical\", \"Parameters used for pre_processing\")\n",
    "        parameters = {\"path\": path,\n",
    "                      \"label_column\": label_column,\n",
    "                      \"assay\": assay,\n",
    "                      \"convert\": convert,\n",
    "                      \"log_normalize\": log_normalize,\n",
    "                      \"scale_data\": scale_data,\n",
    "                      \"quantile_normalize\": quantile_normalize,\n",
    "                      \"save\": save\n",
    "                     }\n",
    "        for key,value in parameters.items():\n",
    "            logger.write(\"critical\", \"{}: {}\".format(key, value))\n",
    "    \n",
    "    data = sc.read(path)\n",
    "    \n",
    "    if log_normalize:\n",
    "        sc.pp.normalize_total(data, target_sum = 1e4)\n",
    "        sc.pp.log1p(data)\n",
    "    \n",
    "    if scale_data:\n",
    "        sc.pp.scale(data)\n",
    "        \n",
    "    if convert:\n",
    "        counts = convert_data(data = data, assay = assay, label_column = label_column)\n",
    "    else:\n",
    "        counts = data.X\n",
    "        \n",
    "    if quantile_normalize:\n",
    "        quant_norm_data = convert_data(data, assay = assay)\n",
    "        counts = quantile_normalize(data.X)\n",
    "    \n",
    "    # convert string label into numeric label\n",
    "    labels = data.obs[label_column].unique().tolist()\n",
    "    labels.sort()\n",
    "    label = data.obs[label_column].apply(lambda x: labels.index(x))\n",
    "    \n",
    "    res = {\"matrix\": counts, \n",
    "           \"convert_label\": label, \n",
    "           \"original_label\": data.obs[label_column], \n",
    "           \"sort_uniq_label\": labels} \n",
    "    \n",
    "    if save:\n",
    "        file_name = \"Preprocessing_data_{}.pkl\".format(record_time())\n",
    "        with open(file_name, \"wb\") as output:\n",
    "            pickle.dump(res, output)\n",
    "    \n",
    "    if logger != \"None\": \n",
    "        logger.write(\"info\", \"finish pre processing\")\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a36edae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fengtang/miniconda3/lib/python3.11/site-packages/anndata/__init__.py:51: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = pre_processing(\"./cellhint_demo_folder/cellhint_demo_folder/Spleen.h5ad\", \n",
    "                      assay = \".X\", \n",
    "                      logger = my_logger,\n",
    "                      label_column = \"cell_type\",\n",
    "                      convert = True,\n",
    "                      scale_data = False,\n",
    "                      save = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85f027d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(data, \n",
    "                      label_column,\n",
    "                      filename,\n",
    "                      logger = None,\n",
    "                      mode = \"ensemble\",\n",
    "                      random_foreast_threshold = None,\n",
    "                      #SVM_threshold = None, \n",
    "                      variance_threshold = \"zero\",\n",
    "                      mutual_info = True, \n",
    "                      chi_square_test = False,\n",
    "                      F_test = False,\n",
    "                      model = \"random_foreast\",\n",
    "                      n_estimators = 100,\n",
    "                      random_state = 10,\n",
    "                      kernel = \"linear\",\n",
    "                      decision_function_shape = \"ovo\",\n",
    "                      n_features_to_select = None,\n",
    "                      step = 100,\n",
    "                      save = True):\n",
    "    \"\"\"\n",
    "    A function to do feature seletion based on filtering, embedding and wrapping method respectively or combing those methods together.\n",
    "    \n",
    "    Parameters:\n",
    "    data: A pandas data frame object.\n",
    "    label_column: The name of cell type column in the data.\n",
    "    logger: A log_file object to write log information into disk. Default is None. \n",
    "    random_foreast_threshold. A float or int value to set the cutoff (feature_importance_) by random foreast model-basedd embedding feature selection.It needs to be specified when model is set in 'random_foreast'. Default is `1 / the number of all features`\n",
    "    variance_threshold: A string to decide which variance cutoff is used to filter out features.\"zero\" or \"median\" could be selected. \n",
    "    mutual_info: Bool value decide whether a mutual information method is employed to filtering out features further.\n",
    "    chi_sqaure_test: A Bool value decide whether a chi square test method is employed to filtering out features further.\n",
    "    F_test: Bool value decide whether a F test method is employed to filtering out features further.\n",
    "    model: String to decide which model is used by embedding-based feature selection. \"random_foreast\", \"logistic\" and \"svm\" could be selected.\n",
    "    n_estimators: The number of trees in the forest.\n",
    "    random_state: Controls both the randomness of the bootstrapping of the samples used when building trees (if ``bootstrap=True``) and the sampling of thefeatures to consider when looking for the best split at each node.\n",
    "    kernel: Specifies the kernel type to be used in the support vector machine algorithm.\n",
    "    decision_function_shape: Whether to return a one-vs-rest ('ovr') decision function of shape (n_samples, n_classes) as all other classifiers, or the originalone-vs-one ('ovo') decision function of libsvm which has shape.(n_samples, n_classes * (n_classes - 1) / 2)\n",
    "    n_featurs_to_selct: int or float, default=None.The number of features to select. If `None`, half of the features are selected. If integer, the parameter is the absolute number of features to select. If float between 0 and 1, it is the fraction of features to select.\n",
    "    step:int or float, default=1. If greater than or equal to 1, then ``step`` corresponds to the (integer) number of features to remove at each iteration. If within (0.0, 1.0), then ``step`` corresponds to the percentage (rounded down) of features to remove at each iteration.\n",
    "    save: Bool value to decide whether write the pre-processed data into the disk.\n",
    "    \"\"\"\n",
    "    message = \"{} {}\".format(record_time(), \"start feature selection\")\n",
    "    print(message)\n",
    "    \n",
    "    if logger != None:\n",
    "        logger.write(\"info\", remove_time(message))\n",
    "    \n",
    "        parameters = {\n",
    "                     \"label_column\": label_column,\n",
    "                     \"file_name\": filename,\n",
    "                     \"mode\": mode,\n",
    "                     \"random_foreast_threshold\": random_foreast_threshold,\n",
    "                     \"variance_threshold\": variance_threshold,\n",
    "                     \"mutual_info\": mutual_info,\n",
    "                     \"chi_square_test\": chi_square_test,\n",
    "                      \"F_test\": F_test,\n",
    "                      \"model\" : model,\n",
    "                      \"n_estimators\": n_estimators,\n",
    "                      \"random_state\": random_state,\n",
    "                      \"kernel\": kernel,\n",
    "                      \"decision_function_shape\": decision_function_shape,\n",
    "                      \"n_features_to_select\": n_features_to_select,\n",
    "                      \"step\" : step,\n",
    "                      \"save\" : save\n",
    "                     }\n",
    "\n",
    "        for key, value in parameters.items():\n",
    "            logger.write(\"critical\", \"{}: {}\".format(key, value))\n",
    "    \n",
    "    # step 1 - convert category label into numeric label\n",
    "    message = \"{} step 1 - converting categoric label into numeric label\".format(record_time())\n",
    "    print(message)\n",
    "    if logger != None:\n",
    "        logger.write(\"info\", remove_time(message))\n",
    "  \n",
    "    \n",
    "    le = LabelEncoder().fit(data[label_column])\n",
    "    label = le.transform(data[label_column])\n",
    "    data[label_column] = label\n",
    "        \n",
    "    X = data.iloc[:, 1:-1]\n",
    "    y = data.iloc[:, -1]\n",
    "    \n",
    "    # step 2 - do feature selection\n",
    "    message = \"{} step 2 - do feature selection\".format(record_time())\n",
    "    print(message)\n",
    "    if logger != None:\n",
    "        logger.write(\"info\", remove_time(message))\n",
    "    \n",
    "    message = \"{} ======== filtering based selection ========\".format(record_time())\n",
    "    print(message)\n",
    "    if logger != None:\n",
    "        logger.write(\"info\", remove_time(message))\n",
    "    \n",
    "        \n",
    "    # filtering-based feature selection\n",
    "    # filter out by variance \n",
    "    if variance_threshold == \"zero\":\n",
    "        var_selector = VarianceThreshold()\n",
    "        X_var = var_selector.fit_transform(X)\n",
    "        retained_features_by_filter = X.columns[var_selector.get_support()]\n",
    "        message = \"* {} {} features remained after filter out features with 0 variance\".format(record_time(), X_var.shape[1])\n",
    "        print(message)\n",
    "        if logger != None:\n",
    "            logger.write(\"info\", remove_time(message))\n",
    "\n",
    "    elif variance_threshold == \"median\":\n",
    "        var_selector = VarianceThreshold(np.median(np.var(np.array(X), axis = 0)))\n",
    "        X_var = var_selector.fit_transform(X)\n",
    "        retained_features_by_filter = X.columns[var_selector.get_support()]\n",
    "        message = \"* {} {} features remained after filter out features below median variance of all features\".format(record_time(), X_var.shape[1])\n",
    "        print(message)\n",
    "        if logger != None:\n",
    "            logger.write(\"info\", remove_time(message))\n",
    "\n",
    "    # filter by chi sqaure test\n",
    "    if chi_square_test and F_test == False:\n",
    "        chivalue, pvalues_chi = chi2(X_var, y)\n",
    "        k = chivalue.shape[0] - (pvalues_chi > 0.05).sum()\n",
    "        selector = SelectKBest(chi2, k = k)\n",
    "        X_fschi = selector.fit_transform(X_var, y)\n",
    "        retained_features_by_filter = retained_features_by_filter[selector.get_support()]\n",
    "        message = \"** {} {} features remained after further chi sqaure test filtering\".format(record_time(), X_fschi.shape[1])\n",
    "        print(message)\n",
    "        if logger != None:\n",
    "            logger.write(\"info\", remove_time(message))\n",
    "\n",
    "    # filter by F test\n",
    "    if F_test and chi_square_test == False:\n",
    "        F, pvalues_f = f_classif(X_var, y)\n",
    "        k = F.shape[0] - (pvalues_f > 0.05).sum()\n",
    "        selector = SelectKBest(f_classif, k = k)\n",
    "        X_fsF = selector.fit_transform(X_var, y)\n",
    "        retained_features_by_filter = retained_features_by_filter[selector.get_support()]\n",
    "        message = \"** {} {} features remained after further chi sqaure test filtering\".format(record_time(), X_fsF.shape[1])\n",
    "        print(message)\n",
    "        if logger != None:\n",
    "            logger.write(\"info\", remove_time(message))\n",
    "\n",
    "    # filter by mutual infomation\n",
    "    if (F_test == False and chi_square_test == False) and mutual_info:\n",
    "        res = mutual_info_classif(X_var, y)\n",
    "        k = res.shape[0] - sum(res <= 0)\n",
    "        selector = SelectKBest(mutual_info_classif, k = k)\n",
    "        X_fsmic = selector.fit_transform(X_var, y)\n",
    "        retained_features_by_filter = retained_features_by_filter[selector.get_support()]\n",
    "        message = \"** {} {} features remained after further mutual information filtering\".format(record_time(), X_fsmic.shape[1])\n",
    "        print(message)\n",
    "        if logger != None:\n",
    "            logger.write(\"info\", remove_time(message))\n",
    "\n",
    "    message = \"{} ======== embedding based selection ========\".format(record_time())\n",
    "    print(message)\n",
    "    if logger != None:\n",
    "        logger.write(\"info\", remove_time(message))\n",
    "\n",
    "    # embedding-based on feature selection\n",
    "    # select by random foreast model\n",
    "    if model == \"random_foreast\":\n",
    "        RFC_ = RandomForestClassifier(n_estimators = n_estimators, random_state = random_state)\n",
    "        # when random_foreast_threshold is None, \n",
    "        #  `1 / number of features` will be used as threshold\n",
    "        if random_foreast_threshold == None:\n",
    "            random_foreast_threshold = 1 / X.shape[1] \n",
    "        RFC_embedding_selector = SelectFromModel(RFC_, threshold = random_foreast_threshold)\n",
    "        X_RFC_embedding = RFC_embedding_selector.fit_transform(X, y)\n",
    "        retained_features_by_embedding = RFC_embedding_selector.get_feature_names_out()\n",
    "        message = \"* {} {} features remained after random foreast based embedding filtering\".format(record_time(), X_RFC_embedding.shape[1])\n",
    "        print(message)\n",
    "        if logger != None:\n",
    "            logger.write(\"info\", remove_time(message))\n",
    "\n",
    "    # select by logistic regression model\n",
    "    elif model == \"logistic\":\n",
    "        logistic_ = LogisticRegression(multi_class = \"multinomial\", random_state = random_state, max_iter=200)\n",
    "        log_embedding_selector = SelectFromModel(logistic_, norm_order = 1)\n",
    "        X_log_embedding = log_embedding_selector.fit_transform(X, y)\n",
    "        retained_features_by_embedding = log_embedding_selector.get_feature_names_out()\n",
    "        message = \"* {} {} features remained after logistic regression based embedding filtering\".format(record_time(), X_log_embedding.shape[1])\n",
    "        print(message)\n",
    "        if logger != None:\n",
    "            logger.write(\"info\", remove_time(message))\n",
    "\n",
    "    # select by SVM model\n",
    "    elif model  == \"svm\":\n",
    "        SVC_ = SVC(decision_function_shape = decision_function_shape, kernel = kernel)\n",
    "        SVC_embedding_selector = SelectFromModel(SVC_, norm_order = 1)\n",
    "        X_SVC_embedding = SVC_embedding_selector.fit_transform(np.array(X), y)\n",
    "        retained_features_by_embedding = X.columns[SVC_embedding_selector.get_support()]\n",
    "        message = \"* {} {} features remained after svm based embedding filtering\".format(record_time(), X_SVC_embedding.shape[1])\n",
    "        print(message)\n",
    "        if logger != None:\n",
    "            logger.write(\"info\", remove_time(message))\n",
    "\n",
    "    # selected by wrapping method\n",
    "    message = \"{} ======== wrapping based selection ========\".format(record_time())\n",
    "    print(message)\n",
    "    if logger != None:\n",
    "        logger.write(\"info\", remove_time(message))\n",
    "    \n",
    "    if n_features_to_select == None:\n",
    "        # when features to select is None,\n",
    "        # 50% of all features will be used as threshold\n",
    "        n_features_to_select = int(X.shape[1] * 0.5)\n",
    "\n",
    "    if model == \"random_foreast\":\n",
    "        RFC_ = RandomForestClassifier(n_estimators = n_estimators, random_state = random_state)\n",
    "        RFC_wrapping_selector = RFE(RFC_, n_features_to_select = n_features_to_select, step = step)\n",
    "        X_RFC_wrapping = RFC_wrapping_selector.fit_transform(X, y)\n",
    "        retained_features_by_wrapping = X.columns[RFC_wrapping_selector.support_]\n",
    "        message = \"* {} {} features remained after RFE - random foreast based wrapping filtering\".format(record_time(), X_RFC_wrapping.shape[1])\n",
    "        print(message)\n",
    "        if logger != None:\n",
    "            logger.write(\"info\", remove_time(message))\n",
    "\n",
    "    elif model == \"logistic\":\n",
    "        logistic_ = LogisticRegression(multi_class = \"multinomial\", random_state = random_state, max_iter=200)\n",
    "        log_wrapping_selector = RFE(logistic_, n_features_to_select = n_features_to_select, step = step)\n",
    "        X_log_wrapping = log_wrapping_selector.fit_transform(X, y)\n",
    "        retained_features_by_wrapping = X.columns[log_wrapping_selector.support_]\n",
    "        message = \"* {} {} features remained after RFE - logistic regression based wrapping filtering\".format(record_time(), X_log_wrapping.shape[1])\n",
    "        print(message)\n",
    "        if logger != None:\n",
    "            logger.write(\"info\", remove_time(message))\n",
    "\n",
    "    elif model == \"svm\":\n",
    "        SVC_ = SVC(decision_function_shape = decision_function_shape, kernel = kernel)\n",
    "        SVC_wrapping_selector = RFE(SVC_, n_features_to_select = n_features_to_select, step = step)\n",
    "        X_SVC_wrapping = SVC_wrapping_selector.fit_transform(np.array(X), y)\n",
    "        retained_features_by_wrapping = X.columns[SVC_wrapping_selector.support_]\n",
    "        message = \"* {} {} features remained after RFE - svm based wrapping filtering\".format(record_time(), X_SVC_wrapping.shape[1])\n",
    "        print(message)\n",
    "        if logger != None:\n",
    "            logger.write(\"info\", remove_time(message))\n",
    "\n",
    "    message = \"{} ======== final feature selection ========\".format(record_time())\n",
    "    print(message)\n",
    "    if logger != None:\n",
    "        logger.write(\"info\", remove_time(message))\n",
    "    \n",
    "    retained_features_by_filter = set(retained_features_by_filter)\n",
    "    retained_features_by_embedding = set(retained_features_by_embedding)\n",
    "    retained_features_by_wrapping = set(retained_features_by_wrapping)\n",
    "    final_feture_selection = reduce(lambda x,y: x.intersection(y), [retained_features_by_embedding, retained_features_by_filter, retained_features_by_wrapping])\n",
    "    message = \"* {} {} features remained after intersecting the key features found by filtering, embedding and wrapping-based feature selection methods\".format(record_time(), len(final_feture_selection))\n",
    "    print(message)\n",
    "    if logger != None:\n",
    "        logger.write(\"info\", remove_time(message))\n",
    "            \n",
    "    output = {\"retained_features_by_filtering\": retained_features_by_filter,\n",
    "             \"retained_features_by_embedding\": retained_features_by_embedding,\n",
    "             \"retained_features_by_wrapping\": retained_features_by_wrapping,\n",
    "             \"final_feature_selection\": final_feture_selection}\n",
    "    \n",
    "    if save == True:\n",
    "        filename = filename + \"_\" + model + \"_\" + \"feature_selection\" + \"_\" + record_time() + \".pkl\"\n",
    "        with open(filename, \"wb\") as file:\n",
    "            pickle.dump(output, file)\n",
    "    \n",
    "    message = \"{} finish feature selection\".format(record_time())\n",
    "    print(message)\n",
    "    if logger != None:\n",
    "        logger.write(\"info\", remove_time(message))\n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df43c9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-02 14:11:53 start feature selection\n",
      "2024-01-02 14:11:53 step 1 - converting categoric label into numeric label\n",
      "2024-01-02 14:11:53 step 2 - do feature selection\n",
      "2024-01-02 14:11:53 ======== filtering based selection ========\n",
      "* 2024-01-02 14:11:53 684 features remained after filter out features below median variance of all features\n",
      "** 2024-01-02 14:11:53 534 features remained after further chi sqaure test filtering\n",
      "2024-01-02 14:11:53 ======== embedding based selection ========\n",
      "* 2024-01-02 14:11:58 363 features remained after random foreast based embedding filtering\n",
      "2024-01-02 14:11:58 ======== wrapping based selection ========\n",
      "* 2024-01-02 14:12:39 684 features remained after RFE - random foreast based wrapping filtering\n",
      "2024-01-02 14:12:39 ======== final feature selection ========\n",
      "* 2024-01-02 14:12:39 324 features remained after intersecting the key features found by filtering, embedding and wrapping-based feature selection methods\n",
      "2024-01-02 14:12:39 finish feature selection\n"
     ]
    }
   ],
   "source": [
    "test = feature_selection(data=data[\"matrix\"].iloc[0:5000, 73000:74370], \n",
    "                  label_column=\"cell_type\", \n",
    "                  filename=\"test\",\n",
    "                  variance_threshold=\"median\",\n",
    "                  model=\"random_foreast\",\n",
    "                  chi_square_test=True,\n",
    "                  F_test=False,\n",
    "                  mutual_info=False,\n",
    "                  n_features_to_select=None,\n",
    "                  logger = my_logger\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38b08696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(data,\n",
    "                   label_column,\n",
    "                   features,\n",
    "                   model,\n",
    "                   logger = None,\n",
    "                   test_size = 0.3,\n",
    "                   random_state = 10,\n",
    "                   cv = 10,\n",
    "                   save = True\n",
    "                  ):\n",
    "    \n",
    "    \"\"\"\n",
    "    A function to do model training.\n",
    "    \n",
    "    Parameters:\n",
    "    d data: A pandas data frame object.\n",
    "    label_column: The name of cell type column in the data.\n",
    "    features: Feaures should be kept for model training in the data.\n",
    "    model: Algorithm to train model. \"random_foreast\", \"svm\" or \"logistic\" could be selected. \n",
    "    test_size: \n",
    "    logger: A log_file object to write log information into disk. Default is None. \n",
    "    test_size: Percentage of data remained for testing model.\n",
    "    random_state:\n",
    "    cv: The number of cross validation for grid serach.\n",
    "    save: Bool value to decide whether the result will be written into disk. Default is True.\n",
    "    \"\"\"\n",
    "    \n",
    "    message = \"{} start model training\".format(record_time())\n",
    "    print(message)\n",
    "    print(\"{} model traning based on {} algorithm\".format(record_time(), model))\n",
    "    \n",
    "    if logger != None:\n",
    "        logger.write(\"info\", remove_time(message))\n",
    "        \n",
    "        parameters = {\"label_column\": label_column,\n",
    "                     \"features\": features,\n",
    "                     \"model\": model,\n",
    "                     \"test_size\": test_size,\n",
    "                     \"random_state\": random_state,\n",
    "                     \"cv\": cv,\n",
    "                     \"save\": save}\n",
    "        \n",
    "        for key,value in parameters.items():\n",
    "            logger.write(\"critical\", \"{}: {}\".format(key, value))\n",
    "        \n",
    "        logger.write(\"info\", \"model traning based on {} algorithm\".format(model))\n",
    "    \n",
    "    X = data.loc[:, data.columns != label_column]\n",
    "    y = data.loc[:, data.columns == label_column]\n",
    "    \n",
    "    le = LabelEncoder().fit(y)\n",
    "    y_trans = le.transform(y)\n",
    "    \n",
    "    # only keep informative features\n",
    "    X = data.loc[:, data.columns.isin(features)] \n",
    "    \n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y_trans, test_size = test_size)\n",
    "    \n",
    "    if model == \"random_foreast\":\n",
    "        \n",
    "        parameters = {\"n_estimators\" : np.arange(10, 101, 10),\n",
    "                     \"criterion\" : [\"gini\", \"entropy\"],\n",
    "                     # \"max_depth\": np.linspace(10, 50, 5),\n",
    "                     \"max_features\": np.linspace(0.2, 1, 5),\n",
    "                     \"min_samples_leaf\": np.arange(10, 300, 20),\n",
    "                     \"min_samples_split\": np.arange(2, 100, 10)}\n",
    "        \n",
    "        message = \"{} grid search below paramters getting the best model\".format(record_time())\n",
    "        print(message)\n",
    "        \n",
    "        if logger != None:\n",
    "            logger.write(\"info\", remove_time(message))\n",
    "        \n",
    "        for key,value in parameters.items():\n",
    "            print(\"{}: {}\".format(key, value))\n",
    "            if logger != None:\n",
    "                logger.write(\"critical\", \"{}: {}\".format(key, value))\n",
    "           \n",
    "        \n",
    "        RFC_ = RandomForestClassifier(random_state = random_state)\n",
    "        GS = GridSearchCV(RFC_, parameters, cv = cv)\n",
    "        GS.fit(Xtrain, Ytrain)\n",
    "        best_parameters = GS.best_params_\n",
    "        best_core = GS.best_score_\n",
    "        score_on_test_data = GS.score(Xtest, Ytest)\n",
    "    \n",
    "    elif model == \"logistic\":\n",
    "        \n",
    "        parameters = {\"penalty\": [\"l1\", \"l2\"],\n",
    "                      \"C\": np.linspace(0,1,5),\n",
    "                      \"multi_class\": [\"ovr\", \"multinomial\"]\n",
    "                     }\n",
    "        \n",
    "        message = \"{} grid search below paramters getting the best model\".format(record_time())\n",
    "        print(message)\n",
    "        \n",
    "        if logger != None:\n",
    "            logger.write(\"info\", remove_time(message))\n",
    "        \n",
    "        for key,value in parameters.items():\n",
    "            print(\"{}: {}\".format(key, value))\n",
    "            if logger != None:\n",
    "                logger.write(\"critical\", \"{}: {}\".format(key, value))\n",
    "        \n",
    "        logistic_ = LogisticRegression()\n",
    "        GS = GridSearchCV(logistic_, parameters, cv = cv)\n",
    "        GS.fit(Xtrain, Ytrain)\n",
    "        best_parameters = GS.best_params_\n",
    "        best_core = GS.best_score_\n",
    "        score_on_test_data = GS.score(Xtest, Ytest)\n",
    "    \n",
    "    elif model == \"svm\":\n",
    "        # scale data for SVM\n",
    "        Xtrain = StandardScaler().fit_transform(np.array(Xtrain))\n",
    "        Xtest = StandardScaler().fit_transform(np.array(Xtest))\n",
    "        \n",
    "        parameters = {\"C\": np.linspace(0.01,30,50),\n",
    "                     \"kernel\": [\"rbf\", \"poly\", \"sigmoid\", \"linear\"],\n",
    "                     \"gamma\": [\"auto\", \"scale\"],\n",
    "                     \"coef0\": np.linspace(0,5,10)}\n",
    "        \n",
    "        message = \"{} grid search below paramters getting the best model\".format(record_time())\n",
    "        print(message)\n",
    "        \n",
    "        if logger != None:\n",
    "            logger.write(\"info\", remove_time(message))\n",
    "        \n",
    "        for key,value in parameters.items():\n",
    "            print(\"{}: {}\".format(key, value))\n",
    "            if logger != None:\n",
    "                logger.write(\"critical\", \"{}: {}\".format(key, value))\n",
    "        \n",
    "        SVM_ = SVC()\n",
    "        GS = GridSearchCV(SVM_, parameters, cv = cv)\n",
    "        GS.fit(Xtrain, Ytrain)\n",
    "        best_parameters = GS.best_params_\n",
    "        best_core = GS.best_score_\n",
    "        score_on_test_data = GS.score(Xtest, Ytest)\n",
    "        \n",
    "    \n",
    "    output = {\"model\" : GS,\n",
    "              \"best_score\": best_core,\n",
    "              \"best_parameters\": best_parameters,\n",
    "              \"score_on_test_data\": score_on_test_data,\n",
    "             \"features_used_for_training\": features}\n",
    "    \n",
    "    if save == True:\n",
    "        filename = model + \"_\" + \"training_model\" + \"_\" + record_time() + \".pkl\"\n",
    "        with open(filename, \"wb\") as file:\n",
    "            pickle.dump(output, file)\n",
    "    \n",
    "    message = \"{} finish model training\".format(record_time())\n",
    "    print(message)\n",
    "    if logger != None:\n",
    "        logger.write(\"info\", remove_time(message))\n",
    "    \n",
    "    return(output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3705b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-02 17:15:00 start model training\n",
      "2024-01-02 17:15:00 model traning based on random_foreast algorithm\n",
      "2024-01-02 17:15:00 grid search below paramters getting the best model\n",
      "n_estimators: [ 10  20  30  40  50  60  70  80  90 100]\n",
      "criterion: ['gini', 'entropy']\n",
      "max_features: [0.2 0.4 0.6 0.8 1. ]\n",
      "min_samples_leaf: [ 10  30  50  70  90 110 130 150 170 190 210 230 250 270 290]\n",
      "min_samples_split: [ 2 12 22 32 42 52 62 72 82 92]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fengtang/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/fengtang/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n",
      "/home/fengtang/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test2 = model_training(data = data[\"matrix\"].iloc[0:500, 73000:74370],\n",
    "                      label_column = \"cell_type\",\n",
    "                      features = test[\"final_feature_selection\"],\n",
    "                      model = \"random_foreast\",\n",
    "                      logger = my_logger,\n",
    "                      cv = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9857560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': GridSearchCV(cv=2, estimator=RandomForestClassifier(random_state=10),\n",
       "              param_grid={'criterion': ['gini', 'entropy'],\n",
       "                          'max_features': array([0.2, 0.4, 0.6, 0.8, 1. ]),\n",
       "                          'n_estimators': array([ 10,  20,  30,  40,  50,  60,  70,  80,  90, 100])}),\n",
       " 'best_score': 0.2914285714285714,\n",
       " 'best_parameters': {'criterion': 'gini',\n",
       "  'max_features': 0.8,\n",
       "  'n_estimators': 30},\n",
       " 'score_on_test_data': 0.26666666666666666,\n",
       " 'features_used_for_training': {'ZADH2',\n",
       "  'ZAP70',\n",
       "  'ZBED5',\n",
       "  'ZBED5-AS1',\n",
       "  'ZBP1',\n",
       "  'ZBTB1',\n",
       "  'ZBTB10',\n",
       "  'ZBTB11',\n",
       "  'ZBTB14',\n",
       "  'ZBTB16',\n",
       "  'ZBTB18',\n",
       "  'ZBTB2',\n",
       "  'ZBTB20',\n",
       "  'ZBTB24',\n",
       "  'ZBTB25',\n",
       "  'ZBTB37',\n",
       "  'ZBTB38',\n",
       "  'ZBTB4',\n",
       "  'ZBTB40',\n",
       "  'ZBTB43',\n",
       "  'ZBTB44',\n",
       "  'ZBTB45',\n",
       "  'ZBTB49',\n",
       "  'ZBTB7A',\n",
       "  'ZBTB7B',\n",
       "  'ZBTB8OS',\n",
       "  'ZC2HC1A',\n",
       "  'ZC3H12A',\n",
       "  'ZC3H12D',\n",
       "  'ZC3H13',\n",
       "  'ZC3H14',\n",
       "  'ZC3H15',\n",
       "  'ZC3H18',\n",
       "  'ZC3H3',\n",
       "  'ZC3H4',\n",
       "  'ZC3H6',\n",
       "  'ZC3H7A',\n",
       "  'ZC3H7B',\n",
       "  'ZC3H8',\n",
       "  'ZC3HAV1',\n",
       "  'ZC3HC1',\n",
       "  'ZCCHC10',\n",
       "  'ZCCHC17',\n",
       "  'ZCCHC3',\n",
       "  'ZCCHC7',\n",
       "  'ZCCHC8',\n",
       "  'ZCCHC9',\n",
       "  'ZCRB1',\n",
       "  'ZCWPW1',\n",
       "  'ZDHHC12',\n",
       "  'ZDHHC13',\n",
       "  'ZDHHC16',\n",
       "  'ZDHHC17',\n",
       "  'ZDHHC18',\n",
       "  'ZDHHC2',\n",
       "  'ZDHHC20',\n",
       "  'ZDHHC21',\n",
       "  'ZDHHC24',\n",
       "  'ZDHHC3',\n",
       "  'ZDHHC4',\n",
       "  'ZDHHC5',\n",
       "  'ZDHHC6',\n",
       "  'ZDHHC7',\n",
       "  'ZEB1',\n",
       "  'ZEB2',\n",
       "  'ZFAND1',\n",
       "  'ZFAND2A',\n",
       "  'ZFAND2B',\n",
       "  'ZFAND3',\n",
       "  'ZFAND5',\n",
       "  'ZFAND6',\n",
       "  'ZFAS1',\n",
       "  'ZFC3H1',\n",
       "  'ZFP14',\n",
       "  'ZFP36',\n",
       "  'ZFP36L1',\n",
       "  'ZFP36L2',\n",
       "  'ZFP62',\n",
       "  'ZFP82',\n",
       "  'ZFP91',\n",
       "  'ZFPL1',\n",
       "  'ZFX',\n",
       "  'ZFYVE16',\n",
       "  'ZFYVE19',\n",
       "  'ZFYVE21',\n",
       "  'ZFYVE27',\n",
       "  'ZFYVE28',\n",
       "  'ZHX1',\n",
       "  'ZHX2',\n",
       "  'ZKSCAN1',\n",
       "  'ZKSCAN2-DT',\n",
       "  'ZKSCAN4',\n",
       "  'ZMAT1',\n",
       "  'ZMAT2',\n",
       "  'ZMAT3',\n",
       "  'ZMAT5',\n",
       "  'ZMIZ1',\n",
       "  'ZMIZ2',\n",
       "  'ZMPSTE24',\n",
       "  'ZMYM1',\n",
       "  'ZMYM2',\n",
       "  'ZMYM3',\n",
       "  'ZMYM4',\n",
       "  'ZMYM5',\n",
       "  'ZMYM6',\n",
       "  'ZMYND11',\n",
       "  'ZMYND8',\n",
       "  'ZNF100',\n",
       "  'ZNF101',\n",
       "  'ZNF106',\n",
       "  'ZNF107',\n",
       "  'ZNF12',\n",
       "  'ZNF131',\n",
       "  'ZNF136',\n",
       "  'ZNF138',\n",
       "  'ZNF14',\n",
       "  'ZNF141',\n",
       "  'ZNF143',\n",
       "  'ZNF146',\n",
       "  'ZNF148',\n",
       "  'ZNF160',\n",
       "  'ZNF174',\n",
       "  'ZNF181',\n",
       "  'ZNF184',\n",
       "  'ZNF189',\n",
       "  'ZNF195',\n",
       "  'ZNF200',\n",
       "  'ZNF207',\n",
       "  'ZNF211',\n",
       "  'ZNF212',\n",
       "  'ZNF213-AS1',\n",
       "  'ZNF217',\n",
       "  'ZNF22',\n",
       "  'ZNF224',\n",
       "  'ZNF226',\n",
       "  'ZNF227',\n",
       "  'ZNF24',\n",
       "  'ZNF253',\n",
       "  'ZNF254',\n",
       "  'ZNF26',\n",
       "  'ZNF263',\n",
       "  'ZNF264',\n",
       "  'ZNF266',\n",
       "  'ZNF267',\n",
       "  'ZNF274',\n",
       "  'ZNF276',\n",
       "  'ZNF277',\n",
       "  'ZNF28',\n",
       "  'ZNF280D',\n",
       "  'ZNF281',\n",
       "  'ZNF292',\n",
       "  'ZNF296',\n",
       "  'ZNF3',\n",
       "  'ZNF302',\n",
       "  'ZNF318',\n",
       "  'ZNF32',\n",
       "  'ZNF324',\n",
       "  'ZNF326',\n",
       "  'ZNF330',\n",
       "  'ZNF331',\n",
       "  'ZNF333',\n",
       "  'ZNF337',\n",
       "  'ZNF33A',\n",
       "  'ZNF33B',\n",
       "  'ZNF346',\n",
       "  'ZNF350',\n",
       "  'ZNF354A',\n",
       "  'ZNF354B',\n",
       "  'ZNF362',\n",
       "  'ZNF37A',\n",
       "  'ZNF385A',\n",
       "  'ZNF394',\n",
       "  'ZNF395',\n",
       "  'ZNF397',\n",
       "  'ZNF407',\n",
       "  'ZNF407-AS1',\n",
       "  'ZNF408',\n",
       "  'ZNF414',\n",
       "  'ZNF419',\n",
       "  'ZNF420',\n",
       "  'ZNF428',\n",
       "  'ZNF429',\n",
       "  'ZNF43',\n",
       "  'ZNF430',\n",
       "  'ZNF431',\n",
       "  'ZNF439',\n",
       "  'ZNF44',\n",
       "  'ZNF444',\n",
       "  'ZNF451',\n",
       "  'ZNF467',\n",
       "  'ZNF468',\n",
       "  'ZNF48',\n",
       "  'ZNF480',\n",
       "  'ZNF487',\n",
       "  'ZNF493',\n",
       "  'ZNF500',\n",
       "  'ZNF506',\n",
       "  'ZNF511',\n",
       "  'ZNF512',\n",
       "  'ZNF514',\n",
       "  'ZNF516',\n",
       "  'ZNF518A',\n",
       "  'ZNF519',\n",
       "  'ZNF524',\n",
       "  'ZNF526',\n",
       "  'ZNF529',\n",
       "  'ZNF532',\n",
       "  'ZNF540',\n",
       "  'ZNF544',\n",
       "  'ZNF550',\n",
       "  'ZNF552',\n",
       "  'ZNF557',\n",
       "  'ZNF559',\n",
       "  'ZNF561',\n",
       "  'ZNF562',\n",
       "  'ZNF563',\n",
       "  'ZNF566',\n",
       "  'ZNF567',\n",
       "  'ZNF568',\n",
       "  'ZNF569',\n",
       "  'ZNF573',\n",
       "  'ZNF574',\n",
       "  'ZNF576',\n",
       "  'ZNF580',\n",
       "  'ZNF581',\n",
       "  'ZNF583',\n",
       "  'ZNF586',\n",
       "  'ZNF587',\n",
       "  'ZNF589',\n",
       "  'ZNF592',\n",
       "  'ZNF593',\n",
       "  'ZNF595',\n",
       "  'ZNF598',\n",
       "  'ZNF600',\n",
       "  'ZNF605',\n",
       "  'ZNF606',\n",
       "  'ZNF609',\n",
       "  'ZNF611',\n",
       "  'ZNF622',\n",
       "  'ZNF626',\n",
       "  'ZNF638',\n",
       "  'ZNF639',\n",
       "  'ZNF641',\n",
       "  'ZNF644',\n",
       "  'ZNF652',\n",
       "  'ZNF654',\n",
       "  'ZNF655',\n",
       "  'ZNF667-AS1',\n",
       "  'ZNF668',\n",
       "  'ZNF671',\n",
       "  'ZNF672',\n",
       "  'ZNF675',\n",
       "  'ZNF677',\n",
       "  'ZNF680',\n",
       "  'ZNF681',\n",
       "  'ZNF683',\n",
       "  'ZNF688',\n",
       "  'ZNF691',\n",
       "  'ZNF692',\n",
       "  'ZNF699',\n",
       "  'ZNF7',\n",
       "  'ZNF701',\n",
       "  'ZNF703',\n",
       "  'ZNF706',\n",
       "  'ZNF708',\n",
       "  'ZNF71',\n",
       "  'ZNF714',\n",
       "  'ZNF720',\n",
       "  'ZNF721',\n",
       "  'ZNF738',\n",
       "  'ZNF740',\n",
       "  'ZNF747',\n",
       "  'ZNF75A',\n",
       "  'ZNF764',\n",
       "  'ZNF766',\n",
       "  'ZNF770',\n",
       "  'ZNF771',\n",
       "  'ZNF780A',\n",
       "  'ZNF785',\n",
       "  'ZNF786',\n",
       "  'ZNF789',\n",
       "  'ZNF790',\n",
       "  'ZNF791',\n",
       "  'ZNF80',\n",
       "  'ZNF800',\n",
       "  'ZNF814',\n",
       "  'ZNF816',\n",
       "  'ZNF821',\n",
       "  'ZNF83',\n",
       "  'ZNF830',\n",
       "  'ZNF831',\n",
       "  'ZNF85',\n",
       "  'ZNF860',\n",
       "  'ZNF865',\n",
       "  'ZNF880',\n",
       "  'ZNF891',\n",
       "  'ZNF91',\n",
       "  'ZNF92',\n",
       "  'ZNFX1',\n",
       "  'ZNHIT1',\n",
       "  'ZNHIT2',\n",
       "  'ZNHIT3',\n",
       "  'ZNHIT6',\n",
       "  'ZNRD1ASP',\n",
       "  'ZNRD2',\n",
       "  'ZNRF1',\n",
       "  'ZPR1',\n",
       "  'ZRANB1',\n",
       "  'ZRSR2',\n",
       "  'ZSCAN16-AS1',\n",
       "  'ZSCAN18',\n",
       "  'ZSCAN32',\n",
       "  'ZSWIM7',\n",
       "  'ZSWIM8',\n",
       "  'ZSWIM9',\n",
       "  'ZUP1',\n",
       "  'ZW10',\n",
       "  'ZWILCH',\n",
       "  'ZWINT',\n",
       "  'ZXDC',\n",
       "  'ZYG11B',\n",
       "  'ZYX',\n",
       "  'ZZEF1',\n",
       "  'ZZZ3'}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
